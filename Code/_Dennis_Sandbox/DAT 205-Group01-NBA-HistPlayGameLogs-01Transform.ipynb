{
 "cells": [
  {
   "source": [
    "# DAT 205 Project - Transform data\n",
    "## By Dennis Hung\n",
    "## Version 1\n",
    "## Code DRAFT 2021-03-27\n",
    "\n",
    "## Code Strucuture\n",
    "### Section 0: Function definitions\n",
    "### Section 1: Import libraries\n",
    "### Section 2: Configuration of variables\n",
    "\n",
    "### Section 3: Load the dataset from file and initial analysis\n",
    "- #### Section 3.1: Load the dataset from file\n",
    "- #### Section 3.2: Initial Analysis\n",
    "\n",
    "### Section 4: Transforming/cleansing the data \n",
    "- #### Section 4.1: Enhance the data\n",
    "- - #### Prepare data enhancement attributes in dataframe (df_TF) and create UID\n",
    "- - #### Create new TeamGameStats dataframe to aggregate data by UID_STG\n",
    "- ### Section 4.2:  NBA Advanced Stat - PIE (Player Impact Efficiency)\n",
    "- ### Section 4.3:  NBA Advanced Stat - PER (Player Efficiency Rating)\n",
    "- ### Section 4.4 Filter data by Team (if specified)\n",
    "- ### Section 4.5: Remove (Stage 1) from dataframe the unwanted numerical/categorical features\n",
    "- ### Section 4.6: Transform categorical feature (WL) using value replace\n",
    "- ### Section 4.7: Transform categorical features using LabelEncoder\n",
    "- ### Using OneHotEncoding (Not Working)\n",
    "- ### Section 4.8: Define TARGET variable and separate into dataframes by season type\n",
    "\n",
    "## Section 5: Analysis - Heat Maps / Correlation Matrices\n",
    "- ### Section 5.1: Plot Heat Maps for Pre Season, Regular Season, and Playoffs\n",
    "- ### Section 5.2: Remove additional unwanted fields based on Heat Map / Correlation Matrix\n",
    "- ### Section 5.3: Re-check Heat Map / Correlation Matrix\n",
    "\n",
    "## Section 6: Modeling and Analysis\n",
    "- ### Section 6.1: Prepare train and test data\n",
    "- ### Section 6.2: Apply Logistic Regression on the split train/test dataset\n",
    "- ### Section 6.3: Apply Decision Tree Classifier on the split train/test dataset\n",
    "- ### Section 6.4: Apply Random Forest Classifier on the split train/test dataset\n",
    "- ### Section 6.5: Apply xgboost on the split train/test dataset\n",
    "- ### (To be removed) Section 6.6: Apply SVM on the split train/test dataset\n",
    "\n",
    "## Section 7: Cross Validation Scores\n",
    "\n",
    "## Section 8: Summary Report\n",
    "\n",
    "## End of Code\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Updates\n",
    "\n",
    "### 2021-03-xx\n",
    "\n",
    "- Adding xgboost and SVM for modeling\n",
    "\n",
    "- Tuning models\n",
    "\n",
    "\n",
    "\n",
    "### 2021-03-27\n",
    "\n",
    "Code fully working to handle \n",
    "\n",
    "- Loading of different files as raw from  \"HistoricalGameLogs_*.csv' or after data enhancement \"DAT205_Output_Enhanced_df_TF *.csv\"\n",
    "\n",
    "- Enable/disable data enhancment process\n",
    "\n",
    "- Filtering by specific team or all the data before performing corelation matrix or model analysis\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Reference\n",
    "\n",
    "#### How to Get NBA Data Using the nba_api Python Module (Beginner). Retrieved from Playing Numbers: \n",
    "\n",
    "https://www.playingnumbers.com/2019/12/how-to-get-nba-data-using-the-nba_api-python-module-beginner/\n",
    "\n",
    "#### Patel, S. (2020, August 19). swar / nba_api. Retrieved from GitHub: \n",
    "\n",
    "https://github.com/swar/nba_api/blob/master/docs/table_of_contents.md\n",
    "\n",
    "#### Issues\n",
    "\n",
    "https://github.com/swar/nba_api/issues/124\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Note: \n",
    "#### For this analysis, this code relies on the CSV output from \"DAT 205-Group01-NBA-HistPlayGameLogs.ipynb\" or the enhanced data from this code as the dataset "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Section 0: Function definitions\n",
    "\n",
    "hms_string(sec_elapsed)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60))/60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h,m,s)\n",
    "\n",
    "# Null field analysis\n",
    "def nullFieldAnalysis(df):\n",
    "    df_missingDataInfo = pd.DataFrame({'Count': df.isnull().sum(), 'Percent': 100*df.isnull().sum()/len(df)})\n",
    "    #Printing the columns with over XX% of missing values (ie 60 = 60%) This is set to 0 for 0%\n",
    "    null_threshold = 0 \n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"==== Null value analysis ====\")\n",
    "    return df_missingDataInfo[df_missingDataInfo['Percent'] > null_threshold].sort_values(by=['Percent'])\n",
    "\n",
    "# CalcThreshold_List\n",
    "def CalcThreshold_List(totalRecords):\n",
    "    CTL_10 = int(round(totalRecords*0.1,0))\n",
    "    CTL_20 = int(round(totalRecords*0.2,0))\n",
    "    CTL_30 = int(round(totalRecords*0.3,0))\n",
    "    CTL_40 = int(round(totalRecords*0.4,0))\n",
    "    CTL_50 = int(round(totalRecords*0.5,0))\n",
    "    CTL_60 = int(round(totalRecords*0.6,0))\n",
    "    CTL_70 = int(round(totalRecords*0.7,0))\n",
    "    CTL_80 = int(round(totalRecords*0.8,0))\n",
    "    CTL_90 = int(round(totalRecords*0.9,0))\n",
    "    CTL_100 = int(round(totalRecords*1,0))\n",
    "\n",
    "    CTL_05 = int(round(totalRecords*0.05,0))\n",
    "    CTL_15 = int(round(totalRecords*0.15,0))\n",
    "    CTL_25 = int(round(totalRecords*0.25,0))\n",
    "    CTL_35 = int(round(totalRecords*0.35,0))\n",
    "    CTL_45 = int(round(totalRecords*0.45,0))\n",
    "    CTL_55 = int(round(totalRecords*0.55,0))\n",
    "    CTL_65 = int(round(totalRecords*0.65,0))\n",
    "    CTL_75 = int(round(totalRecords*0.75,0))\n",
    "    CTL_85 = int(round(totalRecords*0.85,0))\n",
    "    CTL_95 = int(round(totalRecords*0.95,0))\n",
    "\n",
    "    Threshold_List = [1, CTL_05, CTL_10, CTL_15, CTL_20, CTL_25, CTL_30, CTL_35, CTL_40, CTL_45, \n",
    "                    CTL_50, CTL_55, CTL_60, CTL_65, CTL_70, CTL_75, CTL_80, CTL_85, \n",
    "                    CTL_90, CTL_95, CTL_100]\n",
    "    return Threshold_List\n"
   ]
  },
  {
   "source": [
    "# Section 1: Import libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing libraries\n",
    "# pip install xgboost\n",
    "# pip install tpot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialized required packages\n",
    "# Standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Graphing packages\n",
    "import seaborn as sns\n",
    "\n",
    "# import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as mpatches\n",
    "# import matplotlib.lines as mlines\n",
    "\n",
    "# Data preparation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Modeling packages\n",
    "# import tensorflow as tf\n",
    "# import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Regression modeling\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# to fix xgboost warnings error\n",
    "# https://github.com/EpistasisLab/tpot/issues/1139\n",
    "# \"Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\"\n",
    "# from tpot import TPOTClassifier\n",
    "# from tpot.config import classifier_config_dict\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Confusion matrix, Accuracy, sensitivity and specificity\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "# from sklearn.feature_selection import VarianceThreshold \n",
    "# from sklearn.feature_selection import RFE \n",
    "# from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Clustering\n",
    "# from sklearn.datasets import make_blobs\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "# Following code is being deprecated\n",
    "# from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# Initialize variables if there is any debugging required\n",
    "# Insert following line and activate the debugging.\n",
    "# # VALIDATION CODE \n",
    "# if debug_active == 'yes':\n",
    "# \n",
    "# Use \"display(df)\"\" if the result command is \"df\" to retain the same format\n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "source": [
    "# Section 2: Configuration of variables\n",
    "\n",
    "- Must manually set the following variables\n",
    "\n",
    "- gameTypeListed as one of the following: 'Pre Season', 'Regular Season', 'Playoffs'\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# General configuration\n",
    "debug_active = 'yes'\n",
    "loop_max = 100\n",
    "# showNumRecs = 15\n",
    "numFormat = '{:.4f}'\n",
    "numFormat_Pct = \"{:.0%}\"\n",
    "\n",
    "# Data Transformation 'yes' or other\n",
    "# dataEnhancement_active = 'yes'\n",
    "dataEnhancement_active = 'no'\n",
    "    \n",
    "# Section 3.1: Load the dataset from file\n",
    "# pick who is running the code and comment out the others\n",
    "# coder = 'bhavika'\n",
    "# coder = 'cindy'\n",
    "coder = 'dennis'\n",
    "\n",
    "# Setup file name for csv or Excel (.xlsx)\n",
    "if coder == 'bhavika':\n",
    "    filename = 'D:/McMaster/DAT205/Capstone/Data/HistoricalGameLogs_2004-05_to_2019-20_ALL.csv'\n",
    "elif coder == 'dennis':\n",
    "    # filename = './HistoricalGameLogs_2004-05_to_2019-20_ALL.csv'\n",
    "    # filename = './DAT205_Output_Enhanced_df_TF 2004-2020.csv'\n",
    "    # Test Data files\n",
    "    # filename = './HistoricalGameLogs_2007-08_to_2008-09_ALL.csv'\n",
    "    filename = './DAT205_Output_Enhanced_df_TF 2007-09.csv'\n",
    "    \n",
    "\n",
    "# filename = filename + seasonStart + '_to_' + seasonEnd + '_' + gameType + '.csv'\n",
    "# filename = filename + seasonStart + '_to_' + seasonEnd + '_ALL' + '.csv'\n",
    "\n",
    "\n",
    "# Section 4.4 Filter data by Team (if specified)\n",
    "# Filter the dataset by team or None\n",
    "allTeamsList = ['CLE', 'LAC', 'NOH', 'WAS', 'ORL', 'NJN', 'PHX', 'DET', 'IND', \\\n",
    "       'CHA', 'DAL', 'ATL', 'NYK', 'CHI', 'BOS', 'MIN', 'PHI', 'HOU', \\\n",
    "       'POR', 'TOR', 'SAC', 'UTA', 'GSW', 'MIA', 'SEA', 'MEM', 'LAL', \\\n",
    "       'SAS', 'DEN', 'MIL', 'NOK', 'ZAK', 'CHN', 'PAN', 'RMA', 'MMT', \\\n",
    "       'MTA', 'MAL', 'LRO', 'EPT', 'OKC', 'LRY', 'BAR', 'MOS', 'OLP', \\\n",
    "       'PAR', 'LAB', 'MAC', 'MLN', 'BKN', 'FCB', 'RMD', 'MPS', 'EAM', \\\n",
    "       'ALB', 'FBU', 'NOP', 'UBB', 'FLA', 'BAU', 'FEN', 'SLA', 'SDS', \\\n",
    "       'BNE', 'MEL', 'SYD', 'GUA', 'PER', 'ADL', 'NZB', 'BJD', 'FRA']\n",
    "# teamSelected = 'None'\n",
    "teamSelected = 'TOR'\n",
    "\n",
    "# Section 6: Modeling and Analysis\n",
    "random_state_val = 42\n",
    "\n",
    "# Section 6.1: Prepare train and test data\n",
    "# Select a season \n",
    "gameTypeListed = ['Pre Season', 'Regular Season', 'Playoffs']\n",
    "gameTypeListed_code = [0, 1, 2]\n",
    "gameTypeToProcess = 1\n",
    "test_size_val = 0.30\n",
    "\n",
    "# Section 5: Analysis - Heat Maps / Correlation Matrices\n",
    "plotSize = (20,15)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # TEST CODE\n",
    "# # Option 1: For all currently possible seasons\n",
    "# # seasonsListed = ['1946-47', '1947-48', '1948-49', '1949-50'\n",
    "# # , '1950-51', '1951-52', '1952-53', '1953-54', '1954-55', '1955-56', '1956-57', '1957-58', '1958-59', '1959-60'\n",
    "# # , '1960-61', '1961-62', '1962-63', '1963-64', '1964-65', '1965-66', '1966-67', '1967-68', '1968-69', '1969-70'\n",
    "# # , '1970-71', '1971-72', '1972-73', '1973-74', '1974-75', '1975-76', '1976-77', '1977-78', '1978-79', '1979-80'\n",
    "# # , '1980-81', '1981-82', '1982-83', '1983-84', '1984-85', '1985-86', '1986-87', '1987-88', '1988-89', '1989-90'\n",
    "# # , '1990-91', '1991-92', '1992-93', '1993-94', '1994-95', '1995-96', '1996-97', '1997-98', '1998-99', '1999-00'\n",
    "# # , '2000-01', '2001-02', '2002-03', '2003-04', '2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10'\n",
    "# # , '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19', '2019-20'\n",
    "# # , '2020-21']\n",
    "\n",
    "# seasonsListed = ['2004-05', '2005-06', '2006-07', '2007-08', '2008-09', '2009-10'\n",
    "# , '2010-11', '2011-12', '2012-13', '2013-14', '2014-15', '2015-16', '2016-17', '2017-18', '2018-19', '2019-20']\n",
    "\n",
    "# seasonStart = seasonsListed[0]\n",
    "# seasonEnd = seasonsListed[-1]\n",
    "\n",
    "# # Request info for each season in the list\n",
    "# df_gamelogs_player = []\n",
    "# countFirstYear = 0"
   ]
  },
  {
   "source": [
    "# Section 3: Load the dataset from file and initial analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section 3.1: Load the dataset from file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CSV or Excel file \n",
    "# Note the other option in Jupyter Notebook is to use the upload the csv files before running the code\n",
    "\n",
    "# lst of column names which needs to be string\n",
    "lst_str_cols = ['GAME_ID']\n",
    "# use dictionary comprehension to make dict of dtypes\n",
    "dict_dtypes = {x : 'str'  for x in lst_str_cols}\n",
    "# use dict on dtypes\n",
    "df = pd.read_csv(filename, dtype=dict_dtypes)\n",
    "# Excel file import\n",
    "# df = pd.read_excel(filename)\n",
    "\n",
    "# Remove duplicate index from import\n",
    "if dataEnhancement_active == 'yes':\n",
    "    unwanted_list = ['Unnamed: 0']\n",
    "else: \n",
    "    unwanted_list = ['Unnamed: 0', 'UID_STG']\n",
    "\n",
    "X_headers_list = df.columns.tolist()\n",
    "for x in unwanted_list:\n",
    "    X_headers_list.remove(x)\n",
    "\n",
    "# Display current dataframe\n",
    "df_Initial = df[X_headers_list]\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(df_Initial)\n",
    "    # Examine shape of dataframe\n",
    "    display(df_Initial.shape)\n",
    "    # Examine the type of attributes in the dataframe\n",
    "    print(\"Shape of the dataset\")\n",
    "    df_Initial.info()\n",
    "    # Describe the numerical data\n",
    "    df_Initial.describe()\n",
    "    \n"
   ]
  },
  {
   "source": [
    "## Section 3.2: Initial Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the headers of columns that use descriptive or non-numerical values\n",
    "categorical_Features = df_Initial.dtypes[df_Initial.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print(\"VALIDATION CODE\")\n",
    "    print(categorical_Features)\n",
    "\n",
    "# Describe the categorical data\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"==== Description of the categorical features ====\")\n",
    "display(df_Initial[categorical_Features].describe())\n",
    "\n",
    "# # Null field analysis\n",
    "nullFieldAnalysis(df_Initial)\n",
    "# # Null field analysis\n",
    "# df_missingDataInfo = pd.DataFrame({'Count': df_Initial.isnull().sum(), 'Percent': 100*df_Initial.isnull().sum()/len(df)})\n",
    "\n",
    "# #Printing the columns with over XX% of missing values (ie 60 = 60%) This is set to 0 for 0%\n",
    "# null_threshold = 0 \n",
    "# print(\"\")\n",
    "# print(\"\")\n",
    "# print(\"==== Null value analysis ====\")\n",
    "# df_missingDataInfo[df_missingDataInfo['Percent'] > null_threshold].sort_values(by=['Percent'])"
   ]
  },
  {
   "source": [
    "# "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Section 4: Transforming/cleansing the data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data cleansing of nulls (Not working)\n",
    "\n",
    "## Correction to missing PreSeason games WL values only \n",
    "\n",
    "49 PreSeason records \n",
    "\n",
    "2007-08 \n",
    "GAME_ID 0010700072 / 2007-10-19\n",
    "BOS vs NJN   W 36 to L 33\n",
    "\n",
    "2008-09 \n",
    "GAME_ID 0010800035 / 2008-10-11\n",
    "DEN vs PHX   W 77 to L 72\n",
    "Note some player game data seems missing\n",
    "\n",
    "## Corrected missing player name data\n",
    "\n",
    "740 records (727 preseason and 13 regular season)\n",
    "\n",
    "This is not important as the player names are excluded from the analysis\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_missingDataInfo[df_missingDataInfo['Percent'] > null_threshold].loc[df_missingDataInfo[]]\n",
    "\n",
    "# missingDataInfo_List = df_missingDataInfo.loc[df_missingDataInfo['Count']>0].index.tolist()\n",
    "# # ['PLAYER_NAME', 'WL']\n",
    "\n",
    "# numOfRows = len(df_missingDataInfo.index)\n",
    "\n",
    "# for missingDataInfo_Value in missingDataInfo_List:\n",
    "#     df_missingDataInfo[df_missingDataInfo[index] = missingDataInfo_Value]\n",
    "\n",
    "\n",
    "# # Pull player data\n",
    "# from nba_api.stats.static import players\n",
    "# player_dict = players.get_players()\n",
    "\n",
    "\n",
    "# # Use ternary operator or write function \n",
    "# # Names are case sensitive\n",
    "# bron = [player for player in player_dict if player['full_name'] == 'LeBron James'][0]\n",
    "# bron_id = bron['id']\n",
    "\n",
    "# # find team Ids\n",
    "# from nba_api.stats.static import teams \n",
    "# teams = teams.get_teams()\n",
    "# GSW = [x for x in teams if x['full_name'] == 'Golden State Warriors'][0]\n",
    "# GSW_id = GSW['id']"
   ]
  },
  {
   "source": [
    "## Section 4.1: Enhance the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup variables for data transformation\n",
    "df_TF = df_Initial\n",
    "totalNumRec = df_TF.shape[0]\n",
    "\n",
    "# Check df_TeamGameStats\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print(totalNumRec)\n",
    "    display(df_TF)\n",
    "    print(df_TF.columns)"
   ]
  },
  {
   "source": [
    "### Prepare data enhancement attributes in dataframe (df_TF) and create UID "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataEnhancement_active == 'yes':\n",
    "    # Add columns for\n",
    "    #   UID_STG for SEASON_YEAR', 'TEAM_ID', 'GAME_ID'\n",
    "    #   PIE for Performance Impact Efficiency\n",
    "    #   PER for Player Efficieny Rating\n",
    "\n",
    "    addFieldInTFList = ['UID_STG', 'PIE', 'PER']\n",
    "    for addField in addFieldInTFList:\n",
    "        df_TF[addField] = 'new field'\n",
    "\n",
    "    # Reference to sum column values under certain condition.\n",
    "    # https://intellipaat.com/community/49/how-do-i-sum-values-in-a-column-that-match-a-given-condition-using-pandas\n",
    "    # https://cmdlinetips.com/2018/01/how-to-get-unique-values-from-a-column-in-pandas-data-frame/  \n",
    "    # https://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\n",
    "\n",
    "    # # TEST CODE\n",
    "    # df_TF['UID_STG'] = df_TF['UID_STG'].apply(lambda df_TF['UID_STG']: df_TF['SEASON_YEAR'] + str(df_TF['TEAM_ID'] +  df_TF['GAME_ID'])\n",
    "    # df_TF['UID_STG'] = df_TF['UID_STG'].apply(lambda df_TF['UID_STG']: df_TF['SEASON_YEAR'])\n",
    "    # df_TF['PIE'] = df_TF['PIE'].map(lambda df_TF['PIE']: df_TF['SEASON_YEAR'])\n",
    "\n",
    "    Threshold_List = CalcThreshold_List(totalNumRec)\n",
    "    start_time01 = time.time()\n",
    "    print(\"==========================\")\n",
    "    print(\"Data enhancement\")\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "    print(\"Total Records \", totalNumRec)\n",
    "    print(\"\")\n",
    "    print(\"% Completed | Duration (hh:mm:ss) | Record #\")\n",
    "    currNumRecord = 0\n",
    "\n",
    "    for currNumRec in range(totalNumRec):\n",
    "        df_TF['UID_STG'].loc[currNumRec] = df_TF['SEASON_YEAR'].loc[currNumRec] + str(df_TF['TEAM_ID'].loc[currNumRec]) +  df_TF['GAME_ID'].loc[currNumRec]\n",
    "        \n",
    "        currNumRecord = currNumRecord + 1\n",
    "        if currNumRecord in Threshold_List:\n",
    "            currPctCompleted = round((currNumRecord / totalNumRec) *100,1)\n",
    "            time_took01 = time.time() - start_time01\n",
    "            # % completed  |  Processing Time  |  Current Record \n",
    "            print(\"    \",currPctCompleted, \"   |   \", hms_string(time_took01), \"    |   \", currNumRecord)\n",
    "\n",
    "    # Check df_TeamGameStats\n",
    "    # VALIDATION CODE \n",
    "    if debug_active == 'yes':\n",
    "        display(df_TF)"
   ]
  },
  {
   "source": [
    "### Create new TeamGameStats dataframe to aggregate data by UID_STG\n",
    "\n",
    "This is used by PIE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataEnhancement_active == 'yes':\n",
    "    # Create dataframe of team game stats\n",
    "    TeamGameStats_List = ['SEASON_YEAR','TEAM_ABBREVIATION', 'Game_Type', 'GmFGM', 'GmFGA', 'GmFG_PCT', 'GmFG3M', 'GmFG3A', 'GmFG3_PCT', 'GmFTM', 'GmFTA', 'GmFT_PCT', 'GmOREB', 'GmDREB', 'GmREB', 'GmAST', 'GmTOV', 'GmSTL', 'GmBLK', 'GmBLKA', 'GmPF', 'GmPFD', 'GmPTS', 'GmPLUS_MINUS', 'GmDD2', 'GmTD3', 'PIE_Bottom']\n",
    "\n",
    "    # df_TeamGameStats = pd.DataFrame(data, index=index, columns=columns)\n",
    "    df_TeamGameStats = pd.DataFrame(df_TF['UID_STG'].unique(), columns = ['UID_STG'])\n",
    "\n",
    "    for TeamGameStat in TeamGameStats_List:\n",
    "        df_TeamGameStats[TeamGameStat] = ''\n",
    "\n",
    "# if dataEnhancement_active == 'yes':\n",
    "    # Populate the dataframe with TeamGameStats for currNumRec in range(totalNumRec):\n",
    "    totalNumRec_TGS = df_TeamGameStats.shape[0]\n",
    "\n",
    "    TeamGameStat_List = df_TeamGameStats['UID_STG'].tolist()\n",
    "    Threshold_List_TGS = CalcThreshold_List(totalNumRec_TGS)\n",
    "    start_time02 = time.time()\n",
    "    print(\"==========================\")\n",
    "    print(\"Data enhancement\")\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "    print(\"Total Records \", totalNumRec_TGS)\n",
    "    print(\"\")\n",
    "    print(\"% Completed | Duration (hh:mm:ss) | Record #\")\n",
    "    currNumRecord = 0\n",
    "    \n",
    "    # for currTeamGameStat in TeamGameStat_List:  \n",
    "    for currNumRec2 in range(totalNumRec_TGS):    \n",
    "        # Pull UID_STG from df_TeamGameStats \n",
    "        currTeamGameUID = df_TeamGameStats['UID_STG'].loc[currNumRec2]\n",
    "        # Filter df_TF by UID_STG and create temp dataframe for this data\n",
    "        df_TF_Temp = []\n",
    "        df_TF_Temp = df_TF[df_TF['UID_STG']==currTeamGameUID]\n",
    "        # Input in to df_TeamGameStats row with the following calculations for \n",
    "        # 'SEASON_YEAR','TEAM_ABBREVIATION', 'Game_Type', 'GmFGM', 'GmFGA', 'GmFG_PCT', 'GmFG3M', 'GmFG3A', 'GmFG3_PCT', \n",
    "        # 'GmFTM', 'GmFTA', 'GmFT_PCT', 'GmOREB', 'GmDREB', 'GmREB', 'GmAST', 'GmTOV', 'GmSTL', 'GmBLK',\n",
    "        # 'GmBLKA', 'GmPF', 'GmPFD', 'GmPTS', 'GmPLUS_MINUS', 'GmDD2', 'GmTD3'\n",
    "        df_TeamGameStats['SEASON_YEAR'].loc[currNumRec2] = df_TF_Temp['SEASON_YEAR'].unique()[0]\n",
    "        df_TeamGameStats['TEAM_ABBREVIATION'].loc[currNumRec2] = df_TF_Temp['TEAM_ABBREVIATION'].unique()[0]\n",
    "        df_TeamGameStats['Game_Type'].loc[currNumRec2] = df_TF_Temp['Game_Type'].unique()[0]\n",
    "        df_TeamGameStats['GmFGM'].loc[currNumRec2] = df_TF_Temp['FGM'].sum()\n",
    "        df_TeamGameStats['GmFGA'].loc[currNumRec2] = df_TF_Temp['FGA'].sum()\n",
    "        df_TeamGameStats['GmFG_PCT'].loc[currNumRec2] = df_TF_Temp['FGM'].sum() / df_TF_Temp['FGA'].sum()\n",
    "        df_TeamGameStats['GmFG3M'].loc[currNumRec2] = df_TF_Temp['FG3M'].sum()\n",
    "        df_TeamGameStats['GmFG3A'].loc[currNumRec2] = df_TF_Temp['FG3A'].sum()\n",
    "        df_TeamGameStats['GmFG3_PCT'].loc[currNumRec2] = df_TF_Temp['FG3M'].sum() / df_TF_Temp['FG3A'].sum()\n",
    "        df_TeamGameStats['GmFTM'].loc[currNumRec2] = df_TF_Temp['FTM'].sum()\n",
    "        df_TeamGameStats['GmFTA'].loc[currNumRec2] = df_TF_Temp['FTA'].sum()\n",
    "        df_TeamGameStats['GmFT_PCT'].loc[currNumRec2] = df_TF_Temp['FTM'].sum() / df_TF_Temp['FTA'].sum()\n",
    "        df_TeamGameStats['GmOREB'].loc[currNumRec2] = df_TF_Temp['OREB'].sum()\n",
    "        df_TeamGameStats['GmDREB'].loc[currNumRec2] = df_TF_Temp['DREB'].sum()\n",
    "        df_TeamGameStats['GmREB'].loc[currNumRec2] = df_TF_Temp['OREB'].sum() + df_TF_Temp['DREB'].sum()\n",
    "        df_TeamGameStats['GmAST'].loc[currNumRec2] = df_TF_Temp['AST'].sum()\n",
    "        df_TeamGameStats['GmTOV'].loc[currNumRec2] = df_TF_Temp['TOV'].sum()\n",
    "        df_TeamGameStats['GmSTL'].loc[currNumRec2] = df_TF_Temp['STL'].sum()\n",
    "        df_TeamGameStats['GmBLK'].loc[currNumRec2] = df_TF_Temp['BLK'].sum()\n",
    "        df_TeamGameStats['GmBLKA'].loc[currNumRec2] = df_TF_Temp['BLKA'].sum()\n",
    "        df_TeamGameStats['GmPF'].loc[currNumRec2] = df_TF_Temp['PF'].sum()\n",
    "        df_TeamGameStats['GmPFD'].loc[currNumRec2] = df_TF_Temp['PFD'].sum()\n",
    "        df_TeamGameStats['GmPTS'].loc[currNumRec2] = df_TF_Temp['PTS'].sum()\n",
    "        df_TeamGameStats['GmPLUS_MINUS'].loc[currNumRec2] = df_TF_Temp['PLUS_MINUS'].sum() / 5\n",
    "        df_TeamGameStats['GmDD2'].loc[currNumRec2] = df_TF_Temp['DD2'].sum()\n",
    "        df_TeamGameStats['GmTD3'].loc[currNumRec2] = df_TF_Temp['TD3'].sum()\n",
    "        \n",
    "        # PIE_Bottom \n",
    "        PIE_Bottom = (df_TeamGameStats['GmPTS'].loc[currNumRec2] + df_TeamGameStats['GmFGM'].loc[currNumRec2] + df_TeamGameStats['GmFTM'].loc[currNumRec2] \\\n",
    "        - df_TeamGameStats['GmFGA'].loc[currNumRec2] - df_TeamGameStats['GmFTA'].loc[currNumRec2] \\\n",
    "        + df_TeamGameStats['GmDREB'].loc[currNumRec2] + (0.5 * df_TeamGameStats['GmOREB'].loc[currNumRec2]) \\\n",
    "        + df_TeamGameStats['GmAST'].loc[currNumRec2] + df_TeamGameStats['GmSTL'].loc[currNumRec2] + (0.5 * df_TeamGameStats['GmBLK'].loc[currNumRec2]) \\\n",
    "        - df_TeamGameStats['GmPF'].loc[currNumRec2] - df_TeamGameStats['GmTOV'].loc[currNumRec2])\n",
    "        # Insert calculated value into dataframe\n",
    "        df_TeamGameStats['PIE_Bottom'].loc[currNumRec2] = PIE_Bottom\n",
    "        # Counter to show this is still processing\n",
    "        currNumRecord = currNumRecord + 1\n",
    "        if currNumRecord in Threshold_List_TGS:\n",
    "            currPctCompleted = round((currNumRecord / totalNumRec_TGS) *100,1)\n",
    "            time_took02 = time.time() - start_time02\n",
    "            # % completed  |  Processing Time  |  Current Record \n",
    "            print(\"    \",currPctCompleted, \"   |   \", hms_string(time_took02), \"    |   \", currNumRecord)\n",
    "\n",
    "    # Check df_TeamGameStats\n",
    "    # VALIDATION CODE \n",
    "    if debug_active == 'yes':\n",
    "        display(df_TeamGameStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataEnhancement_active == 'yes':\n",
    "    columnsToDropFrom_TGS = ['SEASON_YEAR','TEAM_ABBREVIATION', 'Game_Type', 'GmFGM', 'GmFGA', 'GmFG_PCT', 'GmFG3M' \\\n",
    "    , 'GmFG3A', 'GmFG3_PCT', 'GmFTM', 'GmFTA', 'GmFT_PCT', 'GmOREB', 'GmDREB', 'GmREB', 'GmAST', 'GmTOV', 'GmSTL' \\\n",
    "    , 'GmBLK', 'GmBLKA', 'GmPF', 'GmPFD', 'GmPTS', 'GmPLUS_MINUS', 'GmDD2', 'GmTD3']\n",
    "\n",
    "    for dropColumn_TGS in columnsToDropFrom_TGS:\n",
    "        del df_TeamGameStats[dropColumn_TGS]\n",
    "\n",
    "    # Check df_TeamGameStats\n",
    "    # VALIDATION CODE \n",
    "    if debug_active == 'yes':\n",
    "        display(df_TeamGameStats)"
   ]
  },
  {
   "source": [
    "## Section 4.2:  NBA Advanced Stats \n",
    "\n",
    "#### PIE (Player Impact Estimate)\n",
    "\n",
    "Definition PIE measures a player's overall statistical contribution against the total statistics in games they play in. PIE yields results which are comparable to other advanced statistics (e.g. PER) using a simple formula.\n",
    "\n",
    "Formula (PTS + FGM + FTM - FGA - FTA + DREB + (.5 * OREB) + AST + STL + (.5 * BLK) - PF - TO) / (GmPTS + GmFGM + GmFTM - GmFGA - GmFTA + GmDREB + (.5 * GmOREB) + GmAST + GmSTL + (.5 * GmBLK) - GmPF - GmTO)\n",
    "\n",
    "\n",
    "#### PER (Player Efficiency Rating)\n",
    "\n",
    "PER = (FGM x 85.910 + Steals * 53.897 + 3PTM * 51.757 + FTM x 46.845 + Blocks * 39.190 + Offensive_Reb * 39.190 + Assists * 34.677 + Defensive_Reb x 14.707 - Foul * 17.174 - FT_Miss x 20.091 - FG_Miss * 39.190 - TO * 53.897) * (1 / Minutes)\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CODE\n",
    "\n",
    "# currNumRec = 4\n",
    "# PIE_Top = (df_TF['PTS'].loc[currNumRec] + df_TF['FGM'].loc[currNumRec] + df_TF['FTM'].loc[currNumRec] \\\n",
    "#         - df_TF['FGA'].loc[currNumRec] - df_TF['FTA'].loc[currNumRec] \\\n",
    "#         + df_TF['DREB'].loc[currNumRec] + (0.5 * df_TF['OREB'].loc[currNumRec]) \\\n",
    "#         + df_TF['AST'].loc[currNumRec] + df_TF['STL'].loc[currNumRec] + (0.5 * df_TF['BLK'].loc[currNumRec]) \\\n",
    "#         - df_TF['PF'].loc[currNumRec] - df_TF['TOV'].loc[currNumRec])\n",
    "\n",
    "# Ref_UID_STG_Value = df_TF['UID_STG'].loc[currNumRec]   \n",
    "\n",
    "# df_currTGSs = df_TeamGameStats[df_TeamGameStats['UID_STG']==Ref_UID_STG_Value]\n",
    "\n",
    "# PIE_Bottom = df_currTGSs.iloc[0]['PIE_Bottom']\n",
    "# # PIE_Bottom = (df_currTGSs['GmPTS'] + df_currTGSs['GmFGM'] + df_currTGSs['GmFTM'] \\\n",
    "# #         - df_currTGSs['GmFGA'] - df_currTGSs['GmFTA'] \\\n",
    "# #         + df_currTGSs['GmDREB'] + (0.5 * df_currTGSs['GmOREB']) \\\n",
    "# #         + df_currTGSs['GmAST'] + df_currTGSs['GmSTL'] + (0.5 * df_currTGSs['GmBLK']) \\\n",
    "# #         - df_currTGSs['GmPF'] - df_currTGSs['GmTOV'])\n",
    "# PIE_Calc = PIE_Top / PIE_Bottom\n",
    "\n",
    "# # df_TF['PIE'].loc[currNumRec] = PIE_Calc\n",
    "\n",
    "# print(\"PIE_Top = \", PIE_Top)\n",
    "# print(Ref_UID_STG_Value)\n",
    "# display(df_currTGSs)\n",
    "\n",
    "# print(\"PIE_Bottom = \", PIE_Bottom)\n",
    "# print(\"PIE =\", PIE_Calc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataEnhancement_active == 'yes':\n",
    "    Threshold_List = CalcThreshold_List(totalNumRec)\n",
    "    start_time03 = time.time()\n",
    "    print(\"==========================\")\n",
    "    print(\"Data enhancement\")\n",
    "    print(\"==========================\")\n",
    "    print(\"\")\n",
    "    print(\"Total Records \", totalNumRec)\n",
    "    print(\"\")\n",
    "    print(\"% Completed | Duration (hh:mm:ss) | Record #\")\n",
    "    currNumRecord = 0\n",
    "    \n",
    "    for currNumRec in range(totalNumRec):\n",
    "        # Calculates PIE\n",
    "        # PTS + FGM + FTM - FGA - FTA + DREB + (.5 * OREB) + AST + STL + (.5 * BLK) - PF - TO) / (GmPTS + GmFGM + GmFTM - GmFGA - GmFTA + GmDREB + (.5 * GmOREB) + GmAST + GmSTL + (.5 * GmBLK) - GmPF - GmTO)\n",
    "\n",
    "        PIE_Top = (df_TF['PTS'].loc[currNumRec] + df_TF['FGM'].loc[currNumRec] + df_TF['FTM'].loc[currNumRec] \\\n",
    "            - df_TF['FGA'].loc[currNumRec] - df_TF['FTA'].loc[currNumRec] \\\n",
    "            + df_TF['DREB'].loc[currNumRec] + (0.5 * df_TF['OREB'].loc[currNumRec]) \\\n",
    "            + df_TF['AST'].loc[currNumRec] + df_TF['STL'].loc[currNumRec] + (0.5 * df_TF['BLK'].loc[currNumRec]) \\\n",
    "            - df_TF['PF'].loc[currNumRec] - df_TF['TOV'].loc[currNumRec])\n",
    "        # Get the UID_STG from the Player record df_TF\n",
    "        Ref_UID_STG_Value = df_TF['UID_STG'].loc[currNumRec]   \n",
    "\n",
    "        df_currTGSs = df_TeamGameStats[df_TeamGameStats['UID_STG']==Ref_UID_STG_Value]\n",
    "        PIE_Bottom = df_currTGSs.iloc[0]['PIE_Bottom']\n",
    "        PIE = PIE_Top / PIE_Bottom * 100\n",
    "        df_TF['PIE'].loc[currNumRec] = PIE\n",
    "\n",
    "        # # Calculates PER\n",
    "        # # PER= (FGM * 85.910 + Steals * 53.897+ 3PTM * 51.757 + FTM * 46.845 + Blocks * 39.190 + Offensive_Reb * 39.190 + Assists * 34.677\n",
    "        # #            + Defensive_Reb * 14.707 - Foul * 17.174 - FT_Miss * 20.091 - FG_Miss * 39.190- TO * 53.897) x (1 / Minutes)\n",
    "        if df_TF['MIN'].loc[currNumRec] == 0:\n",
    "            PER = 0\n",
    "        else:\n",
    "            FT_Miss = df_TF['FTA'].loc[currNumRec] - df_TF['FTM'].loc[currNumRec]\n",
    "            FG_Miss = df_TF['FGA'].loc[currNumRec] - df_TF['FGM'].loc[currNumRec]\n",
    "            \n",
    "            PER = (df_TF['FGM'].loc[currNumRec] * 85.910 + df_TF['STL'].loc[currNumRec] * 53.897 \\\n",
    "            + df_TF['FG3M'].loc[currNumRec] * 51.757 + df_TF['FTM'].loc[currNumRec] * 46.845 \\\n",
    "            + df_TF['BLK'].loc[currNumRec] * 39.190 + df_TF['OREB'].loc[currNumRec] * 39.190 \\\n",
    "            + df_TF['AST'].loc[currNumRec] * 34.677 + df_TF['DREB'].loc[currNumRec] * 14.707 \\\n",
    "            - df_TF['PF'].loc[currNumRec] * 17.174 - FT_Miss * 20.091 - FG_Miss * 39.190 \\\n",
    "            - df_TF['TOV'].loc[currNumRec] * 53.897) / df_TF['MIN'].loc[currNumRec]\n",
    "\n",
    "        df_TF['PER'].loc[currNumRec] = PER\n",
    "\n",
    "\n",
    "        # Counter to show this is still processing\n",
    "        currNumRecord = currNumRecord + 1\n",
    "        if currNumRecord in Threshold_List:\n",
    "            currPctCompleted = round((currNumRecord / totalNumRec) *100,1)\n",
    "            time_took03 = time.time() - start_time03\n",
    "            # % completed  |  Processing Time  |  Current Record \n",
    "            print(\"    \",currPctCompleted, \"   |   \", hms_string(time_took03), \"    |   \", currNumRecord)"
   ]
  },
  {
   "source": [
    "## MERGED CODE Section 4.3:  NBA Advanced Stat - PER (Player Efficiency Rating)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGED CODE FOR PER\n",
    "# if dataEnhancement_active == 'yes':\n",
    "#     Threshold_List = CalcThreshold_List(totalNumRec)\n",
    "#     start_time04 = time.time()\n",
    "#     print(\"==========================\")\n",
    "#     print(\"Data enhancement\")\n",
    "#     print(\"==========================\")\n",
    "#     print(\"\")\n",
    "#     print(\"Total Records \", totalNumRec)\n",
    "#     print(\"\")\n",
    "#     print(\"% Completed | Duration (hh:mm:ss) | Record #\")\n",
    "#     currNumRecord = 0\n",
    "#     start_time04 = time.time()\n",
    "#     for currNumRec in range(totalNumRec):\n",
    "#         # PER=[ FGM x 85.910 + Steals x 53.897+ 3PTM x 51.757 + FTM x 46.845 + Blocks x 39.190 + Offensive_Reb x 39.190 + Assists x 34.677\n",
    "#         #            + Defensive_Reb x 14.707 - Foul x 17.174 - FT_Miss x 20.091 - FG_Miss x 39.190- TO x 53.897 ] x (1 / Minutes)\n",
    "#         if df_TF['MIN'].loc[currNumRec] == 0:\n",
    "#             PER = 0\n",
    "#         else:\n",
    "#             FT_Miss = df_TF['FTA'].loc[currNumRec] - df_TF['FTM'].loc[currNumRec]\n",
    "#             FG_Miss = df_TF['FGA'].loc[currNumRec] - df_TF['FGM'].loc[currNumRec]\n",
    "            \n",
    "#             PER = (df_TF['FGM'].loc[currNumRec] * 85.910 + df_TF['STL'].loc[currNumRec] * 53.897 \\\n",
    "#             + df_TF['FG3M'].loc[currNumRec] * 51.757 + df_TF['FTM'].loc[currNumRec] * 46.845 \\\n",
    "#             + df_TF['BLK'].loc[currNumRec] * 39.190 + df_TF['OREB'].loc[currNumRec] * 39.190 \\\n",
    "#             + df_TF['AST'].loc[currNumRec] * 34.677 + df_TF['DREB'].loc[currNumRec] * 14.707 \\\n",
    "#             - df_TF['PF'].loc[currNumRec] * 17.174 - FT_Miss * 20.091 - FG_Miss * 39.190 \\\n",
    "#             - df_TF['TOV'].loc[currNumRec] * 53.897) / df_TF['MIN'].loc[currNumRec]\n",
    "\n",
    "#         df_TF['PER'].loc[currNumRec] = PER\n",
    "\n",
    "#          # Counter to show this is still processing\n",
    "#         currNumRecord = currNumRecord + 1\n",
    "#         if currNumRecord in Threshold_List:\n",
    "#             currPctCompleted = round((currNumRecord / totalNumRec) *100,1)\n",
    "#             time_took04 = time.time() - start_time04\n",
    "#             # % completed  |  Processing Time  |  Current Record \n",
    "#             print(\"    \",currPctCompleted, \"   |   \", hms_string(time_took04), \"    |   \", currNumRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE NOTES FOR PER\n",
    "# # Unadjusted Player Efficientcy Rating (uPER)\n",
    "# uPER = (1 / MIN) *\n",
    "#      [ FG3M\n",
    "#      + (2/3) * AST\n",
    "#      + (2 - factor * (team_AST / team_FG)) * FG\n",
    "#      + (FT *0.5 * (1 + (1 - (team_AST / team_FG)) + (2/3) * (team_AST / team_FG)))\n",
    "#      - VOP * TOV\n",
    "#      - VOP * DRB% * (FGA - FG)\n",
    "#      - VOP * 0.44 * (0.44 + (0.56 * DRB%)) * (FTA - FT)\n",
    "#      + VOP * (1 - DRB%) * (TRB - ORB)\n",
    "#      + VOP * DRB% * ORB\n",
    "#      + VOP * STL\n",
    "#      + VOP * DRB% * BLK\n",
    "#      - PF * ((lg_FT / lg_PF) - 0.44 * (lg_FTA / lg_PF) * VOP) ]\n",
    "\n",
    "# # Insert gameType column and list as one of the values in gameTypeListed\n",
    "#         df_gamelogs_players_currSeason['Game_Type'] = gameType\n",
    "#         if countFirstYear == 0:\n",
    "#             df_gamelogs_players = df_gamelogs_players_currSeason\n",
    "#             countFirstYear = 1\n",
    "#         else:\n",
    "#             # df_gamelogs_players = np.concatenate([df_gamelogs_players, df_gamelogs_players_currSeason])\n",
    "#             df_gamelogs_players = pd.concat([df_gamelogs_players, df_gamelogs_players_currSeason],ignore_index=True)\n",
    "#             # df_gamelogs_players = df_gamelogs_players.append(df_gamelogs_players_currSeason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced data as CSV to avoid reprocessing\n",
    "df_TF.to_csv('DAT205_Output_Enhanced_df_TF.csv') "
   ]
  },
  {
   "source": [
    "## Section 4.4 Filter data by Team (if specified)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If a specific team is selected by 'TEAM_ABBREVIATION' then recreate the dataframe with this filter else use the entire dataset as is.\r\n",
    "if teamSelected in allTeamsList:\r\n",
    "    df_TF = df_TF[df_TF['TEAM_ABBREVIATION']==teamSelected]\r\n",
    "else:\r\n",
    "    df_TF    \r\n",
    "display(df_TF)"
   ]
  },
  {
   "source": [
    "## Section 4.5: Remove (Stage 1) from dataframe the unwanted numerical/categorical features\n",
    "\n",
    "#### Note: if data enhancement done then adjust \n",
    "unwanted_categorical_Features_TF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather current list of features\n",
    "numerical_Features = df_TF.columns.tolist()\n",
    "\n",
    "# All possible features\n",
    "# ['SEASON_YEAR', 'PLAYER_ID', 'PLAYER_NAME', 'TEAM_ID', 'TEAM_ABBREVIATION', 'TEAM_NAME', 'GAME_ID', 'GAME_DATE', 'MATCHUP', 'WL', 'MIN', 'FGM', 'FGA', 'FG_PCT', 'FG3M', 'FG3A', 'FG3_PCT', 'FTM', 'FTA', 'FT_PCT', 'OREB', 'DREB', 'REB', 'AST', 'TOV', 'STL', 'BLK', 'BLKA', 'PF', 'PFD', 'PTS', 'PLUS_MINUS', 'DD2', 'TD3', 'Game_Type']\n",
    "\n",
    "for i in categorical_Features: \n",
    "    numerical_Features.remove(i)\n",
    "\n",
    "# Lists unwanted features\n",
    "unwanted_numerical_Features = ['PLAYER_ID', 'TEAM_ID']\n",
    "unwanted_categorical_Features = ['PLAYER_NAME', 'TEAM_ABBREVIATION', 'TEAM_NAME', 'GAME_ID', 'GAME_DATE', 'MATCHUP']\n",
    "\n",
    "# # if Enchancement done then use this to get rid of what extras you don't want.\n",
    "if dataEnhancement_active == 'yes':\n",
    "    unwanted_categorical_Features_TF = ['UID_STG']\n",
    "\n",
    "# unwanted_list_01 = unwanted_numerical_Features + unwanted_categorical_Features + unwanted_categorical_Features_TF\n",
    "if dataEnhancement_active == 'yes':\n",
    "    unwanted_list_01 = unwanted_numerical_Features + unwanted_categorical_Features + unwanted_categorical_Features_TF\n",
    "else:\n",
    "    unwanted_list_01 = unwanted_numerical_Features + unwanted_categorical_Features\n",
    "X_headers_list = df_TF.columns.tolist()\n",
    "\n",
    "for i in unwanted_list_01:\n",
    "    X_headers_list.remove(i)\n",
    "\n",
    "# Reset new dataframe with desired features\n",
    "df_Reduced = df_TF[X_headers_list]\n",
    "\n",
    "# Remaining attributes\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(X_headers_list)"
   ]
  },
  {
   "source": [
    "## Section 4.6: Transform categorical feature (WL) using value replace"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_categorical_Features = ['WL', 'Game_Type']\n",
    "cleanupValue = {'WL': {'W': 1, 'L': 0}, 'Game_Type': {'Pre Season': 0, 'Regular Season': 1, 'Playoffs': 2}}\n",
    "df_Reduced = df_Reduced.replace(cleanupValue)\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(df_Reduced)"
   ]
  },
  {
   "source": [
    "## Section 4.7: Transform categorical features using LabelEncoder\n",
    "\n",
    "This will work with the reminding categorical values as there is a hierarchy for \n",
    "\n",
    "'SEASON_YEAR' - the more recent the season the more relevant it is where as older data is less valuable\n",
    "\n",
    "'Game_Type' - need to think about this but assume regular season is more important"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select features to encode\n",
    "e_categorical = categorical_Features\n",
    "\n",
    "print(e_categorical)\n",
    "\n",
    "for i in unwanted_categorical_Features:\n",
    "    e_categorical.remove(i)\n",
    "\n",
    "print(unwanted_categorical_Features)\n",
    "\n",
    "for j in cleaned_categorical_Features:\n",
    "    e_categorical.remove(j)\n",
    "\n",
    "print(cleaned_categorical_Features)\n",
    "\n",
    "print(e_categorical)\n",
    "\n",
    "# Reset variable\n",
    "categorical_Features = df_Reduced.dtypes[df_Reduced.dtypes == \"object\"].index.tolist()\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "# cat_list = ['Gender','Education_Level','Marital_Status','Income_Category','Card_Category']\n",
    "# cat_list_code = ['Gender_code','Education_Level_code','Marital_Status_code','Income_Category_code','Card_Category_code']\n",
    "\n",
    "df_Encoded = df_Reduced\n",
    "# df_Encoded = df_Reduced[e_categorical]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LabelEncoding on e_categorical features\n",
    "\n",
    "for k in e_categorical:\n",
    "    val_A = k\n",
    "    val_B = k + '_code'\n",
    "    df_Encoded[(val_B)] = lb_make.fit_transform(df_Encoded[val_A])\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(df_Encoded) #Results in appending a new column to df"
   ]
  },
  {
   "source": [
    "## Using OneHotEncoding (Not Working)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel_cat_features = 'Sex'\n",
    "# df_cat = pd.DataFrame(df[sel_cat_features])\n",
    "# df_cat_dummies = pd.get_dummies(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel_num_features.remove(sel_cat_features)\n",
    "# df_sel_features = pd.concat([df[sel_num_features], df_cat_dummies], axis=1)\n",
    "# df_sel_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(categorical_Features)\n",
    "# print(unwanted_categorical_Features)\n",
    "# display(df_Encoded)\n",
    "# display(df_Reduced[e_categorical])\n",
    "# print(e_categorical)\n",
    "# display(df_Reduced)\n"
   ]
  },
  {
   "source": [
    "## Section 4.8: Define TARGET variable and separate into dataframes by season type\n",
    "\n",
    "Remove (Stage 2) from dataframe the featuree (categorical, Target, and other unwanted)\n",
    "\n",
    "Separating the dataframe by gameTypeListed ('Pre Season', 'Regular Season', 'Playoffs')"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(df_Encoded)\n",
    "    print(e_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure variables\n",
    "# gameTypeListed = ['Pre Season', 'Regular Season', 'Playoffs']\n",
    "# gameTypeListed_code = [0, 1, 2]\n",
    "Y_headers_list1 = ['WL', 'Game_Type']\n",
    "Y_headers_list2 = ['WL']\n",
    "e_categorical = e_categorical + Y_headers_list2\n",
    "\n",
    "# Define the current list of features\n",
    "X_headers_list = df_Encoded.columns.tolist()\n",
    "\n",
    "# Remove LabelEncoded categorical features\n",
    "for k in e_categorical:\n",
    "    X_headers_list.remove(k)\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print(e_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_Reduced2 = df_Encoded[X_headers_list]\n",
    "df_Y_Reduced2 = df_Encoded[Y_headers_list1]\n",
    "cleanDFColumns = ['Game_Type', 'SEASON_YEAR_code']\n",
    "# cleanDFColumns = ['Game_Type']\n",
    "\n",
    "for gameType in gameTypeListed_code:\n",
    "    is_gameType_X = df_X_Reduced2['Game_Type']==gameType\n",
    "    is_gameType_Y = df_Y_Reduced2['Game_Type']==gameType\n",
    "    if gameType == 0:\n",
    "        df_X_PreSeason = df_X_Reduced2[is_gameType_X]\n",
    "        df_X_PreSeason = df_X_PreSeason.drop(cleanDFColumns, axis=1)\n",
    "        df_Y_PreSeason = df_Y_Reduced2[is_gameType_Y]\n",
    "        df_Y_PreSeason = df_Y_PreSeason[Y_headers_list2]\n",
    "    elif gameType == 1:\n",
    "        df_X_RegularSeason = df_X_Reduced2[is_gameType_X]\n",
    "        df_X_RegularSeason = df_X_RegularSeason.drop(cleanDFColumns, axis=1)\n",
    "        df_Y_RegularSeason = df_Y_Reduced2[is_gameType_Y]\n",
    "        df_Y_RegularSeason = df_Y_RegularSeason[Y_headers_list2]\n",
    "    elif gameType == 2:\n",
    "        df_X_Playoffs = df_X_Reduced2[is_gameType_X]\n",
    "        df_X_Playoffs = df_X_Playoffs.drop(cleanDFColumns, axis=1)\n",
    "        df_Y_Playoffs = df_Y_Reduced2[is_gameType_Y]\n",
    "        df_Y_Playoffs = df_Y_Playoffs[Y_headers_list2]\n",
    "\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print(\"\")\n",
    "    print(\"Pre Season\")\n",
    "    display(df_X_PreSeason)\n",
    "    display(df_Y_PreSeason)\n",
    "    print(\"\")\n",
    "    print(\"Regular Season\")\n",
    "    display(df_X_RegularSeason)\n",
    "    display(df_Y_RegularSeason)\n",
    "    print(\"\")\n",
    "    print(\"Playoffs\")\n",
    "    display(df_X_Playoffs)\n",
    "    display(df_Y_Playoffs)"
   ]
  },
  {
   "source": [
    "# Section 5: Analysis - Heat Maps / Correlation Matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section 5.1: Plot Heat Maps for Pre Season, Regular Season, and Playoffs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSize = (20,15)\n",
    "# import seaborn as sns\n",
    "plt.figure(figsize=plotSize)\n",
    "sns.set_context(\"paper\", font_scale=1)\n",
    "\n",
    "# sns.heatmap(df.corr(), annot=True, cmap='Blues',vmin=-1, vmax=1, square=False, linewidths=0.5)\n",
    "print(\"\")\n",
    "print(\"Pre Season\")\n",
    "sns.heatmap(df_X_PreSeason.corr(), annot=True, cmap='Reds',vmin=-1, vmax=1, square=False, linewidths=0.5)\n",
    "# display(df_X_PreSeason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=plotSize)\n",
    "sns.set_context(\"paper\", font_scale=1)\n",
    "print(\"\")\n",
    "print(\"Regular Season\")\n",
    "sns.heatmap(df_X_RegularSeason.corr(), annot=True, cmap='Reds',vmin=-1, vmax=1, square=False, linewidths=0.5)\n",
    "# display(df_X_RegularSeason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=plotSize)\n",
    "sns.set_context(\"paper\", font_scale=1)\n",
    "print(\"\")\n",
    "print(\"Playoffs\")\n",
    "sns.heatmap(df_X_Playoffs.corr(), annot=True, cmap='Reds',vmin=-1, vmax=1, square=False, linewidths=0.5)\n",
    "# display(df_X_Playoffs)"
   ]
  },
  {
   "source": [
    "## Section 5.2: Remove additional unwanted fields based on Heat Map / Correlation Matrix\n",
    "\n",
    "### Note: Adjust features to remove after analyzing the matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysis of heat maps\n",
    "### Removed anything over 0.8\n",
    "#### PTS, FGA, FG3M, FTM, PFD, REB\n",
    "\n",
    "\n",
    "### 2005-2020 TOR\n",
    "\n",
    "### Preseason\n",
    "#### FGM or PTS\n",
    "#### FGA vs PTS\n",
    "#### FGM or FGA\n",
    "#### FTM vs FTA\n",
    "#### PFD vs FTM FTA\n",
    "#### REB vs DREB\n",
    "\n",
    "### Reg Season\n",
    "#### FGM vs PTS\n",
    "#### FGA vs PTS\n",
    "#### FGM vs FGA\n",
    "#### FGA vs MIN\n",
    "#### FG3A vs FG3M\n",
    "#### FTM vs FTA\n",
    "#### FTM vs PFD\n",
    "#### FTA vs PFD\n",
    "#### REB vs DREB\n",
    "\n",
    "\n",
    "### Playoffs\n",
    "#### FGM vs FGA\n",
    "#### FGM vs PTS\n",
    "#### FGA vs PTS\n",
    "#### FGA vs MIN\n",
    "#### FTA vs FTM\n",
    "#### FTM vs PFD\n",
    "#### FTA vs PFD\n",
    "#### REB vs DREB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted/useless features. anything over 80% was removed\n",
    "# unwanted_list_02 = ['PTS', 'FGA', 'FG3M', 'FTM', 'PFD', 'REB']\n",
    "unwanted_list_02 = ['PTS', 'FGA', 'FG3M', 'FTM', 'PFD', 'REB']\n",
    "\n",
    "for gameType in gameTypeListed_code:\n",
    "    if gameType == 0:\n",
    "        df_X_PreSeason = df_X_PreSeason.drop(unwanted_list_02, axis=1)\n",
    "    elif gameType == 1:\n",
    "        df_X_RegularSeason = df_X_RegularSeason.drop(unwanted_list_02, axis=1)\n",
    "    elif gameType == 2:\n",
    "        df_X_Playoffs = df_X_Playoffs.drop(unwanted_list_02, axis=1)\n",
    "\n",
    "\n",
    "# Remaining features and after removal of unwanted features in the dataframes\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(df_X_PreSeason)\n",
    "    display(df_X_RegularSeason)\n",
    "    display(df_X_Playoffs)    "
   ]
  },
  {
   "source": [
    "## Section 5.3: Re-check Heat Map / Correlation Matrix\n",
    "\n",
    "This is only a check but not necessary."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    plt.figure(figsize=plotSize)\n",
    "    sns.set_context(\"paper\", font_scale=1)\n",
    "    print(\"\")\n",
    "    print(\"Pre Season\")\n",
    "    sns.heatmap(df_X_PreSeason.corr(), annot=True, cmap='Reds',vmin=-1, vmax=1, square=False, linewidths=0.5)\n",
    "    # display(df_X_PreSeason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    plt.figure(figsize=plotSize)\n",
    "    sns.set_context(\"paper\", font_scale=1)\n",
    "    print(\"\")\n",
    "    print(\"Regular Season\")\n",
    "    sns.heatmap(df_X_RegularSeason.corr(), annot=True, cmap='Reds',vmin=-1, vmax=1, square=False, linewidths=0.5)\n",
    "    # display(df_X_RegularSeason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    plt.figure(figsize=plotSize)\n",
    "    sns.set_context(\"paper\", font_scale=1)\n",
    "    print(\"\")\n",
    "    print(\"Playoffs\")\n",
    "    sns.heatmap(df_X_Playoffs.corr(), annot=True, cmap='Reds',vmin=-1, vmax=1, square=False, linewidths=0.5)\n",
    "    # display(df_X_Playoffs)"
   ]
  },
  {
   "source": [
    "# Section 6: Modeling and Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Section 6.1: Prepare train and test data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a season \n",
    "# gameTypeListed = ['Pre Season', 'Regular Season', 'Playoffs']\n",
    "# gameTypeListed_code = [0, 1, 2]\n",
    "\n",
    "if gameTypeToProcess == 0:\n",
    "    X = df_X_PreSeason\n",
    "    Y = df_Y_PreSeason\n",
    "elif gameTypeToProcess == 1:\n",
    "    X = df_X_RegularSeason\n",
    "    Y = df_Y_RegularSeason\n",
    "elif gameTypeToProcess == 2:\n",
    "    X = df_X_Playoffs\n",
    "    Y = df_Y_Playoffs\n",
    "\n",
    "# Split the code into training and test dataset 0.7/0.3\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = test_size_val, random_state = random_state_val)\n",
    "\n",
    "selectedSeasonRecordCount = X_train.shape[0] + X_test.shape[0]\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    # Validate the split at a high level\n",
    "    print(X_train.shape,Y_train.shape)\n",
    "    print(X_test.shape,Y_test.shape)\n",
    "    print('Season Type: ', gameTypeToProcess)\n",
    "    df_Encoded.to_csv('DAT205_Output_All.csv') \n",
    "    X_train.to_csv('DAT205_Output_Split_X_train.csv') \n",
    "    X_test.to_csv('DAT205_Output_Split_X_test.csv') \n",
    "    Y_train.to_csv('DAT205_Output_Split_Y_train.csv') \n",
    "    Y_test.to_csv('DAT205_Output_Split_Y_test.csv') \n",
    "    display(X_train)\n",
    "    display(Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(X)\n",
    "    display(Y)"
   ]
  },
  {
   "source": [
    "## Section 6.2: Apply Logistic Regression on the split train/test dataset\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.isnan(df_Encoded.any())\n",
    "# np.isfinite(df_Encoded.all())\n",
    "\n",
    "# np.any(np.isnan(df_Encoded))\n",
    "# np.all(np.isfinite(df_Encoded))"
   ]
  },
  {
   "source": [
    "### Logistic Regression - Base Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Create the model\n",
    "LogRegM = LogisticRegression(C=1, solver='liblinear', random_state = random_state_val)\n",
    "\n",
    "\n",
    "# penalty_list='l2' \n",
    "# dual=False\n",
    "# tol=0.0001 \n",
    "# C_list=1.0\n",
    "# fit_intercept=True\n",
    "# intercept_scaling=1\n",
    "# class_weight=None\n",
    "# random_state=None\n",
    "# solver='lbfgs'\n",
    "# max_iter=100\n",
    "# multi_class='auto'\n",
    "# l1_ratio=None\n",
    "# LogRegM = LogisticRegression(C=1, solver='liblinear', random_state = random_state_val)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "LogRegM.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred = LogRegM.predict(X_test)\n",
    "\n",
    "df_Y_pred = pd.DataFrame(Y_pred, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_LogRegM.csv') \n",
    "\n",
    "# Y_pred = pd.DataFrame(Y_pred, columns = [\"WL_code\"])\n",
    "# df = pd.DataFrame(data=numpy_data, index=[\"row1\", \"row2\"], columns=[\"column1\", \"column2\"])\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(Y_pred)\n",
    "    display(df_Y_pred)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Confusion Matrix Analysis Notes\n",
    "https://www.google.com/search?q=confusion+matrix&rlz=1C1GCEA_enCA849CA849&oq=confusion+&aqs=chrome.1.69i57j0i433l2j0j0i433j0l5.2966j0j7&sourceid=chrome&ie=UTF-8\n",
    "\n",
    "## Analysis score notes \n",
    "https://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Compute your models analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_LogRegM = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_LogRegM = f1_score(Y_test, Y_pred)\n",
    "recall_score_LogRegM = recall_score(Y_test, Y_pred)\n",
    "precision_score_LogRegM = precision_score(Y_test, Y_pred)\n",
    "classification_report_LogRegM = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_LogRegM = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_LogRegM\n",
    "sensitivity_LogRegM = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_LogRegM = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_LogRegM))\n",
    "    print('F1 score:', numFormat.format(f1_score_LogRegM))\n",
    "    print('Recall:', numFormat.format(recall_score_LogRegM))\n",
    "    print('Precision:', numFormat.format(precision_score_LogRegM))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_LogRegM))\n",
    "    print('Specificity : ', numFormat.format(specificity_LogRegM))\n",
    "    print('\\n clasification report:\\n', classification_report_LogRegM)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_LogRegM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importance = LogRegM.coef_[0]\n",
    "array_importance=[]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    array_importance.append(v)\n",
    "    # print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# # Convert to dataframe feature_importances results\n",
    "df_feature_importance_values = pd.DataFrame(array_importance)\n",
    "df_feature_importance_values.rename(columns={0:'Feature Importance'}, inplace=True)\n",
    "\n",
    "# # Convert to dataframe feature labels\n",
    "df_feature_names = pd.DataFrame(list(X.columns))\n",
    "df_feature_names.rename(columns={0:'Feature'}, inplace=True)\n",
    "\n",
    "# # Merge the dataframes for feature labels and feature_importances results\n",
    "df_feature_importance_LogRegM = pd.concat([df_feature_names, df_feature_importance_values], axis=1)\n",
    "# df_feature_importance\n",
    "\n",
    "df_feature_importance_LogRegM.sort_values('Feature Importance', ascending=False, inplace=True)\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    df_feature_importance_LogRegM"
   ]
  },
  {
   "source": [
    "### Logistic Regression - Tuning Random Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_search.py:281: UserWarning: The total space of parameters 36 is smaller than n_iter=100. Running 36 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\Users\\Dennis\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "array([0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n       0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n       1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n       0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n       0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0,\n       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n       0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n       1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n       0, 0, 1], dtype=int64)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "     Y_pred\n0         0\n1         1\n2         1\n3         0\n4         1\n..      ...\n504       0\n505       1\n506       0\n507       0\n508       0\n\n[509 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Y_pred</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>506</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>507</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>508</th>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>509 rows  1 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "\n",
    "# penalty_list='l2' \n",
    "# dual=False\n",
    "# tol=0.0001 \n",
    "# C_list=1.0\n",
    "# fit_intercept=True\n",
    "# intercept_scaling=1\n",
    "# class_weight=None\n",
    "# solver='lbfgs'\n",
    "# max_iter=100\n",
    "# multi_class='auto'\n",
    "# l1_ratio=None\n",
    "\n",
    "# Create the model\n",
    "\n",
    "# Parameters\n",
    "fit_intercept_list = [True, False]\n",
    "dual_list = [True, False]\n",
    "penalty_list = ['l1', 'l2', 'elasticnet']\n",
    "solver_list = ['liblinear', 'sag', 'saga']\n",
    "\n",
    "\n",
    "# Attributes\n",
    "# max_features_list = ['auto', 'sqrt', 'log2', 'None']\n",
    "\n",
    "random_grid_LogRegM = {\n",
    "                    'fit_intercept': fit_intercept_list,\n",
    "                    'dual': dual_list,\n",
    "                    'penalty': penalty_list,\n",
    "                    'solver': solver_list}\n",
    "\n",
    "LogRegM_RSCV = RandomizedSearchCV(estimator = LogRegM, param_distributions = random_grid_LogRegM, n_iter = 100, cv = 3, verbose = 0, random_state = random_state_val)\n",
    "\n",
    "# Train the model\n",
    "LogRegM_RSCV.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred_RSCV = LogRegM_RSCV.predict(X_test)\n",
    "\n",
    "df_Y_pred = pd.DataFrame(Y_pred_RSCV, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_LogRegM_RSCV.csv') \n",
    "\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(Y_pred)\n",
    "    display(df_Y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Best Model Parameters = {'solver': 'sag', 'penalty': 'l2', 'fit_intercept': True, 'dual': False}\nAccuracy: 0.7387\nF1 score: 0.6780\nRecall: 0.6250\nPrecision: 0.7407\nSensitivity :  0.8281\nSpecificity :  0.6250\n\n clasification report:\n               precision    recall  f1-score   support\n\n           0       0.74      0.83      0.78       285\n           1       0.74      0.62      0.68       224\n\n    accuracy                           0.74       509\n   macro avg       0.74      0.73      0.73       509\nweighted avg       0.74      0.74      0.74       509\n\n\n confussion matrix:\n [[236  49]\n [ 84 140]]\n\nImprovement (Accuracy) of -1.57%.\nImprovement (F1_score) of -4.07%.\nImprovement (Sensitivity) of 3.06%.\nImprovement (Specificity) of -8.50%.\n"
     ]
    }
   ],
   "source": [
    "accuracy_score_LogRegM_RSCV = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_LogRegM_RSCV = f1_score(Y_test, Y_pred)\n",
    "recall_score_LogRegM_RSCV = recall_score(Y_test, Y_pred)\n",
    "precision_score_LogRegM_RSCV = precision_score(Y_test, Y_pred)\n",
    "classification_report_LogRegM_RSCV = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_LogRegM_RSCV = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_LogRegM_RSCV\n",
    "sensitivity_LogRegM_RSCV = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_LogRegM_RSCV = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "base_accuracy_score = accuracy_score_LogRegM\n",
    "random_accuracy_score = accuracy_score_LogRegM_RSCV\n",
    "comparison_accuracy_score_LogRegM_RSCV = 100 * (random_accuracy_score - base_accuracy_score) / base_accuracy_score\n",
    "\n",
    "base_f1_score = f1_score_LogRegM\n",
    "random_f1_score = f1_score_LogRegM_RSCV\n",
    "comparison_f1_score_LogRegM_RSCV = 100 * (random_f1_score - base_f1_score) / base_f1_score\n",
    "\n",
    "base_sensitivity = sensitivity_LogRegM\n",
    "random_sensitivity= sensitivity_LogRegM_RSCV\n",
    "comparison_sensitivity_LogRegM_RSCV = 100 * (random_sensitivity - base_sensitivity) / base_sensitivity\n",
    "\n",
    "base_specificity = specificity_LogRegM\n",
    "random_specificity = specificity_LogRegM_RSCV\n",
    "comparison_specificity_LogRegM_RSCV = 100 * (random_specificity - base_specificity) / base_specificity\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Best Model Parameters =', LogRegM_RSCV.best_params_)\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_LogRegM_RSCV))\n",
    "    print('F1 score:', numFormat.format(f1_score_LogRegM_RSCV))\n",
    "    print('Recall:', numFormat.format(recall_score_LogRegM_RSCV))\n",
    "    print('Precision:', numFormat.format(precision_score_LogRegM_RSCV))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_LogRegM_RSCV))\n",
    "    print('Specificity : ', numFormat.format(specificity_LogRegM_RSCV))\n",
    "    print('\\n clasification report:\\n', classification_report_LogRegM_RSCV)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_LogRegM_RSCV)\n",
    "\n",
    "    print('\\nImprovement (Accuracy) of {:0.2f}%.'.format(comparison_accuracy_score_LogRegM_RSCV))\n",
    "    print('Improvement (F1_score) of {:0.2f}%.'.format(comparison_f1_score_LogRegM_RSCV))\n",
    "    print('Improvement (Sensitivity) of {:0.2f}%.'.format(comparison_sensitivity_LogRegM_RSCV))\n",
    "    print('Improvement (Specificity) of {:0.2f}%.'.format(comparison_specificity_LogRegM_RSCV))"
   ]
  },
  {
   "source": [
    "## Section 6.3:  Apply Decision Tree Classifier on the split train/test dataset\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decisiontree#sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Decision Tree Classifier - Base Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "DTM = DecisionTreeClassifier(random_state = random_state_val)\n",
    "\n",
    "# Train the model\n",
    "DTM.fit(X_train, Y_train)\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred = DTM.predict(X_test)\n",
    "\n",
    "df_Y_pred = pd.DataFrame(Y_pred, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_DTM.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_DTM = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_DTM = f1_score(Y_test, Y_pred)\n",
    "recall_score_DTM = recall_score(Y_test, Y_pred)\n",
    "precision_score_DTM = precision_score(Y_test, Y_pred)\n",
    "classification_report_DTM = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_DTM = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_DTM\n",
    "sensitivity_DTM = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_DTM = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Model Parameteres =', DTM.get_params())\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_DTM))\n",
    "    print('F1 score:', numFormat.format(f1_score_DTM))\n",
    "    print('Recall:', numFormat.format(recall_score_DTM))\n",
    "    print('Precision:', numFormat.format(precision_score_DTM))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_DTM))\n",
    "    print('Specificity : ', numFormat.format(specificity_DTM))\n",
    "    print('\\n clasification report:\\n', classification_report_DTM)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importance = DTM.feature_importances_\n",
    "array_importance=[]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    array_importance.append(v)\n",
    "    # print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# # Convert to dataframe feature_importances results\n",
    "df_feature_importance_values = pd.DataFrame(array_importance)\n",
    "df_feature_importance_values.rename(columns={0:'Feature Importance'}, inplace=True)\n",
    "\n",
    "# # Convert to dataframe feature labels\n",
    "df_feature_names = pd.DataFrame(list(X.columns))\n",
    "df_feature_names.rename(columns={0:'Feature'}, inplace=True)\n",
    "\n",
    "# # Merge the dataframes for feature labels and feature_importances results\n",
    "df_feature_importance_DTM = pd.concat([df_feature_names, df_feature_importance_values], axis=1)\n",
    "# df_feature_importance\n",
    "\n",
    "df_feature_importance_DTM.sort_values('Feature Importance', ascending=False, inplace=True)\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    df_feature_importance_DTM"
   ]
  },
  {
   "source": [
    "### Decision Tree Classifier - Tuning Random Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "# Parameters\n",
    "criterion_list = ['gini', 'entropy']\n",
    "splitter_list = ['best', 'random']\n",
    "max_depth_list = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth_list.append(None)\n",
    "min_samples_leaf_list = [1, 2, 4]\n",
    "min_samples_split_list = [2, 4, 5, 6, 8]\n",
    "min_weight_fraction_leaf_list = [0.0, 0.5]\n",
    "\n",
    "# Attributes\n",
    "# max_features_list = ['auto', 'sqrt', 'log2', 'None']\n",
    "\n",
    "random_grid_DTM = {'criterion': criterion_list,\n",
    "                    'splitter': splitter_list,\n",
    "                    'max_depth': max_depth_list,\n",
    "                    'min_samples_leaf': min_samples_leaf_list,\n",
    "                    'min_samples_split': min_samples_split_list,\n",
    "                    'min_weight_fraction_leaf': min_weight_fraction_leaf_list}\n",
    "                    # 'max_features': max_features_list}\n",
    "\n",
    "DTM_RSCV = RandomizedSearchCV(estimator = DTM, param_distributions = random_grid_DTM, n_iter = 100, cv = 3, verbose = 2, random_state = random_state_val)\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "DTM_RSCV.fit(X_train, Y_train)\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred_RSCV = DTM_RSCV.predict(X_test)\n",
    "\n",
    "df_Y_pred = pd.DataFrame(Y_pred_RSCV, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_DTM_RSCV.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_DTM_RSCV = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_DTM_RSCV = f1_score(Y_test, Y_pred)\n",
    "recall_score_DTM_RSCV = recall_score(Y_test, Y_pred)\n",
    "precision_score_DTM_RSCV = precision_score(Y_test, Y_pred)\n",
    "classification_report_DTM_RSCV = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_DTM_RSCV = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_DTM_RSCV\n",
    "sensitivity_DTM_RSCV = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_DTM_RSCV = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "base_accuracy_score = accuracy_score_DTM\n",
    "random_accuracy_score = accuracy_score_DTM_RSCV\n",
    "comparison_accuracy_score_DTM_RSCV = 100 * (random_accuracy_score - base_accuracy_score) / base_accuracy_score\n",
    "\n",
    "base_f1_score = f1_score_DTM\n",
    "random_f1_score = f1_score_DTM_RSCV\n",
    "comparison_f1_score_DTM_RSCV = 100 * (random_f1_score - base_f1_score) / base_f1_score\n",
    "\n",
    "base_sensitivity = sensitivity_DTM\n",
    "random_sensitivity= sensitivity_DTM_RSCV\n",
    "comparison_sensitivity_DTM_RSCV = 100 * (random_sensitivity - base_sensitivity) / base_sensitivity\n",
    "\n",
    "base_specificity = specificity_DTM\n",
    "random_specificity = specificity_DTM_RSCV\n",
    "comparison_specificity_DTM_RSCV = 100 * (random_specificity - base_specificity) / base_specificity\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Best Model Parameters =', DTM_RSCV.best_params_)\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_DTM_RSCV))\n",
    "    print('F1 score:', numFormat.format(f1_score_DTM_RSCV))\n",
    "    print('Recall:', numFormat.format(recall_score_DTM_RSCV))\n",
    "    print('Precision:', numFormat.format(precision_score_DTM_RSCV))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_DTM_RSCV))\n",
    "    print('Specificity : ', numFormat.format(specificity_DTM_RSCV))\n",
    "    print('\\n clasification report:\\n', classification_report_DTM_RSCV)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_DTM_RSCV)\n",
    "\n",
    "    print('\\nImprovement (Accuracy) of {:0.2f}%.'.format(comparison_accuracy_score_DTM_RSCV))\n",
    "    print('Improvement (F1_score) of {:0.2f}%.'.format(comparison_f1_score_DTM_RSCV))\n",
    "    print('Improvement (Sensitivity) of {:0.2f}%.'.format(comparison_sensitivity_DTM_RSCV))\n",
    "    print('Improvement (Specificity) of {:0.2f}%.'.format(comparison_specificity_DTM_RSCV))\n"
   ]
  },
  {
   "source": [
    "## Section 6.4: Apply Random Forest Classifier on the split train/test dataset\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)[source]\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "\n",
    "\n",
    "Hint from processing from TPOT under XGBoost\n",
    "\n",
    "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.4, min_samples_leaf=13, min_samples_split=13, n_estimators=100)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Random Forest Classifier - Base Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "RFM = RandomForestClassifier(max_depth=2, random_state=random_state_val)\n",
    "\n",
    "# Train the model\n",
    "RFM.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred = RFM.predict(X_test)\n",
    "df_Y_pred = pd.DataFrame(Y_pred, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_RFM.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_RFM = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_RFM = f1_score(Y_test, Y_pred)\n",
    "recall_score_RFM = recall_score(Y_test, Y_pred)\n",
    "precision_score_RFM = precision_score(Y_test, Y_pred)\n",
    "classification_report_RFM = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_RFM = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_RFM\n",
    "sensitivity_RFM = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_RFM = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Model Parameteres =', RFM.get_params())\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_RFM))\n",
    "    print('F1 score:', numFormat.format(f1_score_RFM))\n",
    "    print('Recall:', numFormat.format(recall_score_RFM))\n",
    "    print('Precision:', numFormat.format(precision_score_RFM))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_RFM))\n",
    "    print('Specificity : ', numFormat.format(specificity_RFM))\n",
    "    print('\\n clasification report:\\n', classification_report_RFM)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_RFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importance = RFM.feature_importances_\n",
    "array_importance=[]\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    array_importance.append(v)\n",
    "    # print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# # Convert to dataframe feature_importances results\n",
    "df_feature_importance_values = pd.DataFrame(array_importance)\n",
    "df_feature_importance_values.rename(columns={0:'Feature Importance'}, inplace=True)\n",
    "\n",
    "# # Convert to dataframe feature labels\n",
    "df_feature_names = pd.DataFrame(list(X.columns))\n",
    "df_feature_names.rename(columns={0:'Feature'}, inplace=True)\n",
    "\n",
    "# # Merge the dataframes for feature labels and feature_importances results\n",
    "df_feature_importance_RFM = pd.concat([df_feature_names, df_feature_importance_values], axis=1)\n",
    "# df_feature_importance\n",
    "\n",
    "df_feature_importance_RFM.sort_values('Feature Importance', ascending=False, inplace=True)\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    df_feature_importance_RFM"
   ]
  },
  {
   "source": [
    "### Random Forest Classifier - Tuning Random Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# RFM = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "\n",
    "# n_estimators_list = [100, 10, 20, 30, 40, 50]\n",
    "n_estimators_list = [int(x) for x in np.linspace(start = 10, stop = 1000, num = 10)]\n",
    "criterion_list = ['gini', 'entropy']\n",
    "# max_depth_list = [None, 1, 5, 10]\n",
    "max_depth_list = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth_list.append(None)\n",
    "min_samples_leaf_list = [1, 2, 4]\n",
    "min_samples_split_list = [2, 4, 5, 6, 8]\n",
    "# min_weight_fraction_leaf_list = 0.0,\n",
    "# max_features_list = ['int', 'float', 'auto', 'sqrt', 'log2', 'None']\n",
    "bootstrap_list = [True, False]\n",
    "\n",
    "random_grid_RTM = {'n_estimators': n_estimators_list,\n",
    "               'criterion': criterion_list,\n",
    "               'max_depth': max_depth_list,\n",
    "               'min_samples_leaf': min_samples_leaf_list,\n",
    "               'min_samples_split': min_samples_split_list,\n",
    "            #    'max_features': max_features_list,\n",
    "               'bootstrap': bootstrap_list}\n",
    "\n",
    "RFM_RSCV = RandomizedSearchCV(estimator = RFM, param_distributions = random_grid_RTM, n_iter = 100, cv = 3, verbose = 2, random_state = random_state_val)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "RFM_RSCV.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred = RFM_RSCV.predict(X_test)\n",
    "df_Y_pred = pd.DataFrame(Y_pred, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_RFM_RSCV.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_RFM_RSCV = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_RFM_RSCV = f1_score(Y_test, Y_pred)\n",
    "recall_score_RFM_RSCV = recall_score(Y_test, Y_pred)\n",
    "precision_score_RFM_RSCV = precision_score(Y_test, Y_pred)\n",
    "classification_report_RFM_RSCV = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_RFM_RSCV = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_RFM_RSCV\n",
    "sensitivity_RFM_RSCV = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_RFM_RSCV = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "base_accuracy_score = accuracy_score_RFM\n",
    "random_accuracy_score = accuracy_score_RFM_RSCV\n",
    "comparison_accuracy_score_RFM_RSCV = 100 * (random_accuracy_score - base_accuracy_score) / base_accuracy_score\n",
    "\n",
    "base_f1_score = f1_score_RFM\n",
    "random_f1_score = f1_score_RFM_RSCV\n",
    "comparison_f1_score_RFM_RSCV = 100 * (random_f1_score - base_f1_score) / base_f1_score\n",
    "\n",
    "base_sensitivity = sensitivity_RFM\n",
    "random_sensitivity= sensitivity_RFM_RSCV\n",
    "comparison_sensitivity_RFM_RSCV = 100 * (random_sensitivity - base_sensitivity) / base_sensitivity\n",
    "\n",
    "base_specificity = specificity_RFM\n",
    "random_specificity = specificity_RFM_RSCV\n",
    "comparison_specificity_RFM_RSCV = 100 * (random_specificity - base_specificity) / base_specificity\n",
    "\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Best Model Parameters =', RFM_RSCV.best_params_)\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_RFM_RSCV))\n",
    "    print('F1 score:', numFormat.format(f1_score_RFM_RSCV))\n",
    "    print('Recall:', numFormat.format(recall_score_RFM_RSCV))\n",
    "    print('Precision:', numFormat.format(precision_score_RFM_RSCV))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_RFM_RSCV))\n",
    "    print('Specificity : ', numFormat.format(specificity_RFM_RSCV))\n",
    "    print('\\n clasification report:\\n', classification_report_RFM_RSCV)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_RFM_RSCV)\n",
    "    \n",
    "    print('\\nImprovement (Accuracy) of {:0.2f}%.'.format(comparison_accuracy_score_RFM_RSCV))\n",
    "    print('Improvement (F1_score) of {:0.2f}%.'.format(comparison_f1_score_RFM_RSCV))\n",
    "    print('Improvement (Sensitivity) of {:0.2f}%.'.format(comparison_sensitivity_RFM_RSCV))\n",
    "    print('Improvement (Specificity) of {:0.2f}%.'.format(comparison_specificity_RFM_RSCV))\n"
   ]
  },
  {
   "source": [
    "## Section 6.5: Apply xgboost on the split train/test dataset\n",
    "\n",
    "Documentation \n",
    "\n",
    "https://xgboost.readthedocs.io/_/downloads/en/release_1.3.0/pdf/\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "\n",
    "# # TPOT \n",
    "# classifier_config_dict['xgboost.XGBClassifier'] = {\n",
    "#     'n_estimators': [100],\n",
    "#     'max_depth': range(1, 11),\n",
    "#     'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "#     'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "#     'min_child_weight': range(1, 21),\n",
    "#     'n_jobs': [1], # replace \"nthread\"\n",
    "#     'verbosity': [0] # add this line to slient warning message\n",
    "# }\n",
    "# XGBM = TPOTClassifier(generations=2, population_size=10, verbosity=2,\n",
    "#                       config_dict=classifier_config_dict)\n",
    "# # XGBoost\n",
    "# Parameters xgb.XGBClassifier(booster=['gbtree', 'dart', 'gblinear'], nthread=[Default is max number of threads] \n",
    "# Parameter for tree boosting \n",
    "\n",
    "XGBM = xgb.XGBClassifier(booster='gbtree')\n",
    "\n",
    "# Train the model\n",
    "# # TPOT \n",
    "# XGBM.fit(X_train, Y_train.values.ravel())\n",
    "# # XGBoost\n",
    "XGBM.fit(X_train, Y_train.values.ravel())\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred = XGBM.predict(X_test)\n",
    "\n",
    "df_Y_pred = pd.DataFrame(Y_pred, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_XGBM.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_XGBM = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_XGBM = f1_score(Y_test, Y_pred)\n",
    "recall_score_XGBM = recall_score(Y_test, Y_pred)\n",
    "precision_score_XGBM = precision_score(Y_test, Y_pred)\n",
    "classification_report_XGBM = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_XGBM = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_XGBM\n",
    "sensitivity_XGBM = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_XGBM = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_XGBM))\n",
    "    print('F1 score:', numFormat.format(f1_score_XGBM))\n",
    "    print('Recall:', numFormat.format(recall_score_XGBM))\n",
    "    print('Precision:', numFormat.format(precision_score_XGBM))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_XGBM))\n",
    "    print('Specificity : ', numFormat.format(specificity_XGBM))\n",
    "    print('\\n clasification report:\\n', classification_report_XGBM)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_XGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importance = XGBM.feature_importances_\n",
    "array_importance=[]\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    array_importance.append(v)\n",
    "    # print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "\n",
    "# # Convert to dataframe feature_importances results\n",
    "df_feature_importance_values = pd.DataFrame(array_importance)\n",
    "df_feature_importance_values.rename(columns={0:'Feature Importance'}, inplace=True)\n",
    "\n",
    "# # Convert to dataframe feature labels\n",
    "df_feature_names = pd.DataFrame(list(X.columns))\n",
    "df_feature_names.rename(columns={0:'Feature'}, inplace=True)\n",
    "\n",
    "# # Merge the dataframes for feature labels and feature_importances results\n",
    "df_feature_importance_XGBM = pd.concat([df_feature_names, df_feature_importance_values], axis=1)\n",
    "# df_feature_importance\n",
    "\n",
    "df_feature_importance_XGBM.sort_values('Feature Importance', ascending=False, inplace=True)\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    df_feature_importance_XGBM"
   ]
  },
  {
   "source": [
    "## (To be removed) Section 6.6: Apply SVM on the split train/test dataset\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "SVCM = SVC()\n",
    "\n",
    "# Train the model\n",
    "SVCM.fit(X_train, Y_train)\n",
    "\n",
    "# Predict using test data\n",
    "Y_pred = SVCM.predict(X_test)\n",
    "\n",
    "df_Y_pred = pd.DataFrame(Y_pred, columns = ['Y_pred'])\n",
    "df_Y_pred.to_csv('DAT205_Output_Y_pred_SVCM.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_SVCM = accuracy_score(Y_test, Y_pred)\n",
    "f1_score_SVCM= f1_score(Y_test, Y_pred)\n",
    "recall_score_SVCM = recall_score(Y_test, Y_pred)\n",
    "precision_score_SVCM = precision_score(Y_test, Y_pred)\n",
    "classification_report_SVCM = classification_report(Y_test, Y_pred)\n",
    "confusion_matrix_SVCM = confusion_matrix(Y_test, Y_pred)\n",
    "cm = confusion_matrix_SVCM\n",
    "sensitivity_SVCM = cm[0,0]/(cm[0,0]+cm[0,1])\n",
    "specificity_SVCM = cm[1,1]/(cm[1,0]+cm[1,1])\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    print('Accuracy:', numFormat.format(accuracy_score_SVCM))\n",
    "    print('F1 score:', numFormat.format(f1_score_SVCM))\n",
    "    print('Recall:', numFormat.format(recall_score_SVCM))\n",
    "    print('Precision:', numFormat.format(precision_score_SVCM))\n",
    "    print('Sensitivity : ', numFormat.format(sensitivity_SVCM))\n",
    "    print('Specificity : ', numFormat.format(specificity_SVCM))\n",
    "    print('\\n clasification report:\\n', classification_report_SVCM)\n",
    "    print('\\n confussion matrix:\\n',confusion_matrix_SVCM)"
   ]
  },
  {
   "source": [
    "# Section 7: Cross Validation Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# seed = random_state_val\n",
    "# # seed = 0\n",
    "\n",
    "# loan_models = []\n",
    "# loan_models.append(('Logistic Regression', LogRegM))\n",
    "# loan_models.append(('Decision Tree', DTM))\n",
    "# loan_models.append(('Decision Tree', DTM_RSCV))\n",
    "# loan_models.append(('Random Forest', RFM))\n",
    "# loan_models.append(('Random Forest', RFM_RSCV))\n",
    "# loan_models.append(('SVM', SVCM))\n",
    "# loan_models.append(('XGBoost', XGBM))\n",
    "\n",
    "\n",
    "# cross_val_scores = []\n",
    "# model_keys = []\n",
    "\n",
    "# df_cross_val_score = []\n",
    "# df_cross_val_score_headers = [0,1,2,3]\n",
    "# df_cross_val_score = pd.DataFrame (df_cross_val_score, columns = df_cross_val_score_headers)\n",
    "\n",
    "# # VALIDATION CODE \n",
    "# # if debug_active == 'yes':\n",
    "# #     nullFieldAnalysis(df_cross_val_score)\n",
    "\n",
    "# df_Addscore = []\n",
    "# scoring = 'accuracy'\n",
    "# for model_key, loan_model in loan_models:\n",
    "#     kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "#     cross_val_score = model_selection.cross_val_score(loan_model, X_train, Y_train.values.ravel(), cv=kfold, scoring=scoring)\n",
    "#     cross_val_scores.append(cross_val_score)\n",
    "#     model_keys.append(model_key)\n",
    "#     msg = \"%s: cross val mean -> %f , cross val std -> %f, kfold variance -> %f\" % (model_key, cross_val_score.mean(), cross_val_score.std(), cross_val_score.var())\n",
    "#     df_Addscore = pd.Series([model_key, cross_val_score.mean(), cross_val_score.std(), cross_val_score.var()])\n",
    "#     df_cross_val_score = df_cross_val_score.append(df_Addscore, ignore_index=True)\n",
    "#     print(msg)\n",
    "\n",
    "# df_cross_val_score.columns = ['Model_Key','Cross_Value_Score_Mean','Cross_Value_Score_STD','Cross_Value_Score_Var']\n",
    "# # VALIDATION CODE \n",
    "# if debug_active == 'yes':\n",
    "#     print(\"\")\n",
    "#     print(\"VALIDATION RESULT\")\n",
    "#     display(df_cross_val_score)\n",
    "#     print(\"\\nmodel_keys \", model_keys)\n",
    "#     print(\"\\ncross_val_scores \", cross_val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # WORK IN PROGRESS\n",
    "\n",
    "# # boxplot algorithm comparison\n",
    "\n",
    "# # plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "# # fig = plt.figure()\n",
    "# # fig.suptitle('Model Comparison')\n",
    "# # ax = fig.add_subplot(111)\n",
    "# # sns.boxplot(x = model_keys, y=cross_val_scores, palette = 'Blues')\n",
    "# # plt.show()\n",
    "\n",
    "# # plt.style.use('ggplot')\n",
    "# plotX = pd.Series(model_keys)\n",
    "# plotY = pd.Series(cross_val_scores)\n",
    "\n",
    "# df = pd.DataFrame({\"model_keys\" : plotX, \"cross_val_scores\" : plotY})\n",
    "# # plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "# # fig = plt.figure()\n",
    "# # fig.suptitle('Model Comparison')\n",
    "# # ax = fig.add_subplot(111)\n",
    "# sns.set(style=\"whitegrid\")\n",
    "# sns.boxplot(data=df, palette = 'Blues')\n",
    "\n",
    "\n",
    "\n",
    "# # plt.style.use('ggplot')\n",
    "# # dummyData.groupby(['quarter', 'brand'])\\\n",
    "# #       .brand.count().unstack().plot.bar(legend=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CODE\n",
    "\n",
    "# print(model_keys)\n",
    "# print(cross_val_scores)\n",
    "\n",
    "\n",
    "# print(plotX)\n",
    "# print(plotY)"
   ]
  },
  {
   "source": [
    "# Section 8: Summary Report"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table of metric analysis\n",
    "df_Metrics = []\n",
    "\n",
    "df_Metrics_headers = [0,1,2,3,4,5,6]\n",
    "df_Metrics = pd.DataFrame (df_Metrics, columns = df_Metrics_headers)\n",
    "\n",
    "df_AddModel = pd.Series(['Logistic Regression', accuracy_score_LogRegM, f1_score_LogRegM, recall_score_LogRegM, precision_score_LogRegM, sensitivity_LogRegM, specificity_LogRegM])\n",
    "df_Metrics = df_Metrics.append(df_AddModel, ignore_index=True)\n",
    "\n",
    "df_AddModel = pd.Series(['Decision Tree',accuracy_score_DTM, f1_score_DTM, recall_score_DTM, precision_score_DTM, sensitivity_DTM, specificity_DTM])\n",
    "df_Metrics = df_Metrics.append(df_AddModel, ignore_index=True)\n",
    "\n",
    "df_AddModel = pd.Series(['Decision Tree (Tuned)',accuracy_score_DTM_RSCV, f1_score_DTM_RSCV, recall_score_DTM_RSCV, precision_score_DTM_RSCV, sensitivity_DTM_RSCV, specificity_DTM_RSCV])\n",
    "df_Metrics = df_Metrics.append(df_AddModel, ignore_index=True)\n",
    "\n",
    "df_AddModel = pd.Series(['Random Forest', accuracy_score_RFM, f1_score_RFM, recall_score_RFM, precision_score_RFM, sensitivity_RFM, specificity_RFM])\n",
    "df_Metrics = df_Metrics.append(df_AddModel, ignore_index=True)\n",
    "\n",
    "df_AddModel = pd.Series(['Random Forest (Tuned)',accuracy_score_RFM_RSCV, f1_score_RFM_RSCV, recall_score_RFM_RSCV, precision_score_RFM_RSCV, sensitivity_RFM_RSCV, specificity_RFM_RSCV])\n",
    "df_Metrics = df_Metrics.append(df_AddModel, ignore_index=True)\n",
    "\n",
    "df_AddModel = pd.Series(['XGBoost',accuracy_score_XGBM, f1_score_XGBM, recall_score_XGBM, precision_score_XGBM, sensitivity_XGBM, specificity_XGBM])\n",
    "df_Metrics = df_Metrics.append(df_AddModel, ignore_index=True)\n",
    "\n",
    "df_AddModel = pd.Series(['SVM',accuracy_score_SVCM, f1_score_SVCM, recall_score_SVCM, precision_score_SVCM, sensitivity_SVCM, specificity_SVCM])\n",
    "df_Metrics = df_Metrics.append(df_AddModel, ignore_index=True)\n",
    "\n",
    "df_Metrics.columns = ['Model','Accuracy','F1 score','Recall','Precision','Sensitivity','Specificity']\n",
    "\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(df_Metrics)\n",
    "\n",
    "# Join dataframes for Metrics and cross_val_scores\n",
    "# df_Summary = pd.concat([df_Metrics,df_cross_val_score], axis=1)\n",
    "df_Summary = df_Metrics\n",
    "# VALIDATION CODE \n",
    "if debug_active == 'yes':\n",
    "    display(df_Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clean up columns by dropping columns of duplicate data (Model_Key)\n",
    "# df_Summary.drop(['Model_Key'], axis=1, inplace=True)\n",
    "# # df_Summary.drop(columns=['Model_Key'], inplace=True)\n",
    "# # VALIDATION CODE \n",
    "# if debug_active == 'yes':\n",
    "#     display(df_Summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"================= Results Summary ==================\\n\")\n",
    "\n",
    "print(\"\\n==================== Configuration ======================\")\n",
    "print('Filter by Team Selected = ', teamSelected)\n",
    "\n",
    "print('Total Number of Records (Initial Dataset) = ', totalNumRec)\n",
    "print('Total Number of Records (Transformed and Filtered Dataset) = ', df_TF.shape[0])\n",
    "print('Game Type Processed (0 = PreSeason / 1 = RegularSeason / 2 = Playoffs) = ', gameTypeToProcess)\n",
    "print('Selected Season Records = ', selectedSeasonRecordCount)\n",
    "print('Train / Test Split = ', test_size_val)\n",
    "print('Model random_state_val = ', random_state_val)\n",
    "\n",
    "print(\"\\n==================== Features ======================\")\n",
    "print('----------------- Removed Features -----------------')\n",
    "display(unwanted_list_01)\n",
    "print('\\n------ Removed attributes - Heat Map / Correlation Matrix ---- ')\n",
    "display(unwanted_list_02)\n",
    "\n",
    "print('\\n------------------- Applied Features --------------------')\n",
    "display(X_test.columns.tolist())\n",
    "\n",
    "print(\"\\n ================= Model Analysis Summary ==================\\n\")\n",
    "display(df_Summary)\n",
    "\n",
    "print('\\n\\n----------------- Logistic Regression --------------------')\n",
    "print('Accuracy:', numFormat.format(accuracy_score_LogRegM))\n",
    "print('F1 score:', numFormat.format(f1_score_LogRegM))\n",
    "print('Recall:', numFormat.format(recall_score_LogRegM))\n",
    "print('Precision:', numFormat.format(precision_score_LogRegM))\n",
    "print('Sensitivity : ', numFormat.format(sensitivity_LogRegM))\n",
    "print('Specificity : ', numFormat.format(specificity_LogRegM))\n",
    "\n",
    "print('\\n clasification report:\\n', classification_report_LogRegM)\n",
    "print('\\n confussion matrix:\\n',confusion_matrix_LogRegM)\n",
    "print(\"\\nFeature Importance\")\n",
    "display(df_feature_importance_LogRegM)\n",
    "\n",
    "print('\\n\\n-------------------- Decision Tree -----------------------\\n')\n",
    "print('Accuracy:', numFormat.format(accuracy_score_DTM))\n",
    "print('F1 score:', numFormat.format(f1_score_DTM))\n",
    "print('Recall:', numFormat.format(recall_score_DTM))\n",
    "print('Precision:', numFormat.format(precision_score_DTM))\n",
    "print('Sensitivity : ', numFormat.format(sensitivity_DTM))\n",
    "print('Specificity : ', numFormat.format(specificity_DTM))\n",
    "\n",
    "print('\\n clasification report:\\n', classification_report_DTM)\n",
    "print('\\n confussion matrix:\\n',confusion_matrix_DTM)\n",
    "print(\"\\nFeature Importance\")\n",
    "display(df_feature_importance_DTM)\n",
    "\n",
    "print('\\n\\n-------------------- Random Forest -----------------------\\n')\n",
    "print('Base Model')\n",
    "print('Accuracy:', numFormat.format(accuracy_score_RFM))\n",
    "print('F1 score:', numFormat.format(f1_score_RFM))\n",
    "print('Recall:', numFormat.format(recall_score_RFM))\n",
    "print('Precision:', numFormat.format(precision_score_RFM))\n",
    "print('Sensitivity : ', numFormat.format(sensitivity_RFM))\n",
    "print('Specificity : ', numFormat.format(specificity_RFM))\n",
    "print('\\n clasification report:\\n', classification_report_RFM)\n",
    "print('\\n confussion matrix:\\n',confusion_matrix_RFM)\n",
    "\n",
    "print('Tuned Model')\n",
    "print('Accuracy:', numFormat.format(accuracy_score_RFM_RSCV))\n",
    "print('F1 score:', numFormat.format(f1_score_RFM_RSCV))\n",
    "print('Recall:', numFormat.format(recall_score_RFM_RSCV))\n",
    "print('Precision:', numFormat.format(precision_score_RFM_RSCV))\n",
    "print('Sensitivity : ', numFormat.format(sensitivity_RFM_RSCV))\n",
    "print('Specificity : ', numFormat.format(specificity_RFM_RSCV))\n",
    "print('\\n clasification report:\\n', classification_report_RFM_RSCV)\n",
    "print('\\n confussion matrix:\\n',confusion_matrix_RFM_RSCV)\n",
    "\n",
    "print('\\nImprovement (Accuracy) of {:0.2f}%.'.format(comparison_accuracy_score_RFM_RSCV))\n",
    "print('Improvement (F1_score) of {:0.2f}%.'.format(comparison_f1_score_RFM_RSCV))\n",
    "print('Improvement (Sensitivity) of {:0.2f}%.'.format(comparison_sensitivity_RFM_RSCV))\n",
    "print('Improvement (Specificity) of {:0.2f}%.'.format(comparison_specificity_RFM_RSCV))\n",
    "\n",
    "print(\"\\nFeature Importance\")\n",
    "display(df_feature_importance_RFM)\n",
    "\n",
    "print('\\n\\n----------------- XGBoost --------------------')\n",
    "print('Accuracy:', numFormat.format(accuracy_score_XGBM))\n",
    "print('F1 score:', numFormat.format(f1_score_XGBM))\n",
    "print('Recall:', numFormat.format(recall_score_XGBM))\n",
    "print('Precision:', numFormat.format(precision_score_XGBM))\n",
    "print('Sensitivity : ', numFormat.format(sensitivity_XGBM))\n",
    "print('Specificity : ', numFormat.format(specificity_XGBM))\n",
    "\n",
    "print('\\n clasification report:\\n', classification_report_XGBM)\n",
    "print('\\n confussion matrix:\\n',confusion_matrix_XGBM)\n",
    "print(\"\\nFeature Importance\")\n",
    "display(df_feature_importance_XGBM)\n",
    "\n",
    "print('\\n\\n----------------- SVM --------------------')\n",
    "print('Accuracy:', numFormat.format(accuracy_score_SVCM))\n",
    "print('F1 score:', numFormat.format(f1_score_SVCM))\n",
    "print('Recall:', numFormat.format(recall_score_SVCM))\n",
    "print('Precision:', numFormat.format(precision_score_SVCM))\n",
    "print('Sensitivity : ', numFormat.format(sensitivity_SVCM))\n",
    "print('Specificity : ', numFormat.format(specificity_SVCM))\n",
    "\n",
    "print('\\n clasification report:\\n', classification_report_SVCM)\n",
    "print('\\n confussion matrix:\\n',confusion_matrix_SVCM)\n",
    "print(\"\\nFeature Importance\")\n",
    "# display(df_feature_importance_SVCM)\n",
    "print(\"Work in progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_took = time.time() - start_time\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(f\"Total Runtime: {hms_string(time_took)}\")\n",
    "if dataEnhancement_active == 'yes':\n",
    "    print(f\"Add Enhancement Columns Runtime: {hms_string(time_took01)}\")\n",
    "    print(f\"Create temp TeamGameStats dataframe Runtime: {hms_string(time_took02)}\")\n",
    "    print(f\"Calculate PIE / PER Runtime: {hms_string(time_took03)}\")\n",
    "    # print(f\"Calculate PER Runtime: {hms_string(time_took04)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}